"""
üöÄ PROFESSIONAL TRADING BOT v5.0 - ENTERPRISE GRADE BACKEND
‚úÖ 11+ Exchange Support (Binance, Bybit, OKX, KuCoin, Gate.io, MEXC, Kraken, Bitfinex, Huobi, Coinbase, Bitget)
‚úÖ Real Data Only - ZERO Synthetic/Fake Data Fallback
‚úÖ Volatility Adjusted Trail System (ICT Compliant)
‚úÖ Advanced ML Ensemble (LSTM + Transformer + LightGBM)
‚úÖ 12+ Candlestick Patterns + ICT Market Structure
‚úÖ Circuit Breaker Pattern for Exchange Failures
‚úÖ Smart Cache with TTL & Source Validation
‚úÖ WebSocket Real-Time Streaming
‚úÖ Enterprise Logging & Monitoring
‚úÖ Rate Limiting & Security Hardening
‚úÖ Data Quality Validation Pipeline
"""

# ========== IMPORTS ==========
import os
import sys
import json
import time
import asyncio
import hashlib
import secrets
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple, Any, Set
from enum import Enum
from collections import defaultdict, deque
import logging
from logging.handlers import RotatingFileHandler

# FastAPI and dependencies
from fastapi import (
    FastAPI,
    Request,
    HTTPException,
    Query,
    WebSocket,
    WebSocketDisconnect,
    status,
    Depends
)
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.staticfiles import StaticFiles

# Async HTTP client
import aiohttp
from aiohttp import ClientTimeout, TCPConnector

# Cache and rate limiting
import redis.asyncio as redis
from fastapi_limiter import FastAPILimiter
from fastapi_limiter.depends import RateLimiter

# Data processing
import pandas as pd
import numpy as np

# ML libraries (optional imports)
ML_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    from sklearn.preprocessing import StandardScaler
    import lightgbm as lgb
    from sklearn.metrics import accuracy_score
    ML_AVAILABLE = True
    logger.info("‚úÖ ML libraries imported successfully")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è ML libraries not available: {e}")

# ========== LOGGING SETUP ==========
def setup_logging():
    """Enterprise-grade logging configuration"""
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    
    # Main application logger
    logger = logging.getLogger("trading_bot")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    
    # Console handler with colors
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    
    class ColoredFormatter(logging.Formatter):
        """Custom formatter with colors for console output"""
        COLORS = {
            'DEBUG': '\033[36m',  # Cyan
            'INFO': '\033[32m',   # Green
            'WARNING': '\033[33m', # Yellow
            'ERROR': '\033[31m',   # Red
            'CRITICAL': '\033[41m', # Red background
            'RESET': '\033[0m'
        }
        
        def format(self, record):
            levelname = record.levelname
            if levelname in self.COLORS:
                record.levelname = f"{self.COLORS[levelname]}{levelname:8s}{self.COLORS['RESET']}"
                record.msg = f"{self.COLORS[levelname]}{record.msg}{self.COLORS['RESET']}"
            return super().format(record)
    
    console_formatter = ColoredFormatter(
        '%(asctime)s | %(levelname)-8s | %(name)-15s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    
    # File handler (rotating, 10MB each, keep 5 files)
    file_handler = RotatingFileHandler(
        os.path.join(log_dir, 'trading_bot.log'),
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter(
        '%(asctime)s | %(levelname)-8s | %(name)-15s | %(funcName)-20s | %(lineno)4d | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)
    
    # Error handler (separate file for errors)
    error_handler = RotatingFileHandler(
        os.path.join(log_dir, 'errors.log'),
        maxBytes=10*1024*1024,
        backupCount=5,
        encoding='utf-8'
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(file_formatter)
    logger.addHandler(error_handler)
    
    # Reduce noise from third-party libraries
    logging.getLogger("aiohttp").setLevel(logging.WARNING)
    logging.getLogger("asyncio").setLevel(logging.WARNING)
    if ML_AVAILABLE:
        logging.getLogger("lightgbm").setLevel(logging.WARNING)
    
    return logger

logger = setup_logging()

# ========== CONFIGURATION ==========
class Config:
    """Centralized configuration management"""
    
    # Security
    API_KEY = os.getenv("TRADING_BOT_API_KEY", secrets.token_urlsafe(32))
    SECRET_KEY = os.getenv("SECRET_KEY", secrets.token_urlsafe(64))
    
    # Rate limiting
    RATE_LIMIT_PER_MINUTE = 60
    RATE_LIMIT_PER_HOUR = 1000
    
    # Caching
    CACHE_TTL_SECONDS = 60  # 1 minute for price data
    CACHE_MAX_SIZE = 1000
    
    # Exchange settings
    EXCHANGE_TIMEOUT = 10  # seconds
    EXCHANGE_MAX_RETRIES = 2
    CIRCUIT_BREAKER_FAILURE_THRESHOLD = 5
    CIRCUIT_BREAKER_RESET_TIMEOUT = 30  # seconds
    
    # Data validation
    MIN_CANDLES_FOR_ANALYSIS = 50
    MIN_CANDLES_FOR_TRAINING = 500
    MIN_DATA_QUALITY_SCORE = 0.7
    
    # ML settings
    ML_SEQUENCE_LENGTH = 60
    ML_FEATURES_MIN = 30
    
    # Production safety
    ALLOW_SYNTHETIC_DATA = False  # HARD POLICY: NEVER ALLOW SYNTHETIC DATA
    REQUIRE_MIN_EXCHANGES = 2  # Require data from at least 2 exchanges
    
    # Environment
    ENV = os.getenv("ENV", "production")
    DEBUG = os.getenv("DEBUG", "false").lower() == "true"
    
    # WebSocket settings
    WS_HEARTBEAT_INTERVAL = 30  # seconds
    
    # Dashboard settings
    DASHBOARD_REFRESH_INTERVAL = 5  # seconds
    
    @classmethod
    def validate(cls):
        """Validate configuration"""
        if cls.ENV == "production" and cls.ALLOW_SYNTHETIC_DATA:
            raise ValueError("SYNTHETIC DATA NOT ALLOWED IN PRODUCTION")
        if cls.REQUIRE_MIN_EXCHANGES < 1:
            raise ValueError("REQUIRE_MIN_EXCHANGES must be at least 1")
        logger.info(f"‚úÖ Configuration validated for {cls.ENV} environment")

Config.validate()

# ========== FASTAPI APPLICATION ==========
app = FastAPI(
    title="Professional AI Trading Bot v5.0",
    description="Enterprise-grade trading system with real data only policy",
    version="5.0.0",
    docs_url="/docs" if Config.DEBUG else None,
    redoc_url="/redoc" if Config.DEBUG else None,
    openapi_url="/openapi.json" if Config.DEBUG else None,
    contact={
        "name": "Trading Bot Support",
        "email": "support@tradingbot.com"
    },
    license_info={
        "name": "Proprietary",
        "url": "https://tradingbot.com/license"
    }
)

# Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"] if Config.DEBUG else ["https://yourdomain.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["X-Request-ID", "X-RateLimit-Limit", "X-RateLimit-Remaining"],
)

app.add_middleware(GZipMiddleware, minimum_size=1000)

# Security headers middleware
@app.middleware("http")
async def add_security_headers(request: Request, call_next):
    response = await call_next(request)
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
    response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline' https://cdn.jsdelivr.net; style-src 'self' 'unsafe-inline' https://cdn.jsdelivr.net https://fonts.googleapis.com; font-src 'self' https://fonts.gstatic.com; img-src 'self' data: https:; connect-src 'self' wss://* https://*"
    response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    return response

# Templates
templates = Jinja2Templates(directory="templates")

# ========== ENUMERATIONS ==========
class SignalType(str, Enum):
    STRONG_BUY = "STRONG_BUY"
    BUY = "BUY"
    NEUTRAL = "NEUTRAL"
    SELL = "SELL"
    STRONG_SELL = "STRONG_SELL"

class Timeframe(str, Enum):
    M1 = "1m"
    M5 = "5m"
    M15 = "15m"
    M30 = "30m"
    H1 = "1h"
    H4 = "4h"
    D1 = "1d"
    W1 = "1w"
    MN1 = "1M"

class CircuitBreakerState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

# ========== CIRCUIT BREAKER ==========
class ExchangeCircuitBreaker:
    """Circuit breaker pattern for exchange failure isolation"""
    
    def __init__(self, failure_threshold: int = 5, reset_timeout: int = 30):
        self.failure_threshold = failure_threshold
        self.reset_timeout = reset_timeout
        self.failures: Dict[str, int] = defaultdict(int)
        self.state: Dict[str, CircuitBreakerState] = defaultdict(lambda: CircuitBreakerState.CLOSED)
        self.last_failure_time: Dict[str, float] = {}
        self.last_success_time: Dict[str, float] = {}
    
    def allow_request(self, exchange_name: str) -> bool:
        """Check if request is allowed based on circuit state"""
        current_state = self.state[exchange_name]
        
        if current_state == CircuitBreakerState.CLOSED:
            return True
        
        if current_state == CircuitBreakerState.OPEN:
            # Check if reset timeout has passed
            if time.time() - self.last_failure_time.get(exchange_name, 0) > self.reset_timeout:
                self.state[exchange_name] = CircuitBreakerState.HALF_OPEN
                logger.info(f"üîÑ Circuit breaker half-open for {exchange_name}")
                return True
            return False
        
        # HALF_OPEN state - allow one test request
        return True
    
    def record_success(self, exchange_name: str):
        """Record successful request"""
        self.failures[exchange_name] = 0
        self.state[exchange_name] = CircuitBreakerState.CLOSED
        self.last_success_time[exchange_name] = time.time()
        logger.debug(f"‚úÖ Circuit breaker closed for {exchange_name}")
    
    def record_failure(self, exchange_name: str):
        """Record failed request"""
        self.failures[exchange_name] += 1
        self.last_failure_time[exchange_name] = time.time()
        
        if self.failures[exchange_name] >= self.failure_threshold:
            self.state[exchange_name] = CircuitBreakerState.OPEN
            logger.warning(f"‚ö†Ô∏è Circuit breaker OPENED for {exchange_name} after {self.failures[exchange_name]} failures")
    
    def get_status(self, exchange_name: str) -> Dict[str, Any]:
        """Get circuit breaker status for exchange"""
        return {
            "state": self.state[exchange_name].value,
            "failure_count": self.failures[exchange_name],
            "last_failure": self.last_failure_time.get(exchange_name),
            "last_success": self.last_success_time.get(exchange_name)
        }

circuit_breaker = ExchangeCircuitBreaker(
    failure_threshold=Config.CIRCUIT_BREAKER_FAILURE_THRESHOLD,
    reset_timeout=Config.CIRCUIT_BREAKER_RESET_TIMEOUT
)

# ========== PRICE FETCHER ==========
class PriceFetcher:
    """Enterprise-grade multi-exchange data fetcher"""
    
    EXCHANGES = [
        # Tier 1 - Highest priority
        {
            "name": "Binance",
            "priority": 1,
            "weight": 1.0,
            "symbol_fmt": lambda s: s.replace("/", ""),
            "endpoint": "https://api.binance.com/api/v3/klines",
            "params_builder": lambda s, i, l: {"symbol": s, "interval": i, "limit": l}
        },
        {
            "name": "Bybit",
            "priority": 2,
            "weight": 0.95,
            "symbol_fmt": lambda s: s.replace("/", ""),
            "endpoint": "https://api.bybit.com/v5/market/kline",
            "params_builder": lambda s, i, l: {"category": "spot", "symbol": s, "interval": i, "limit": l}
        },
        {
            "name": "OKX",
            "priority": 3,
            "weight": 0.9,
            "symbol_fmt": lambda s: s.replace("/", "-"),
            "endpoint": "https://www.okx.com/api/v5/market/candles",
            "params_builder": lambda s, i, l: {"instId": s, "bar": i, "limit": l}
        },
        # Tier 2
        {
            "name": "KuCoin",
            "priority": 4,
            "weight": 0.85,
            "symbol_fmt": lambda s: s.replace("/", "-"),
            "endpoint": "https://api.kucoin.com/api/v1/market/candles",
            "params_builder": lambda s, i, l: {"symbol": s, "type": i}
        },
        {
            "name": "Gate.io",
            "priority": 5,
            "weight": 0.8,
            "symbol_fmt": lambda s: s.replace("/", "_"),
            "endpoint": "https://api.gateio.ws/api/v4/spot/candlesticks",
            "params_builder": lambda s, i, l: {"currency_pair": s, "interval": i, "limit": l}
        },
        # Tier 3
        {
            "name": "MEXC",
            "priority": 6,
            "weight": 0.75,
            "symbol_fmt": lambda s: s.replace("/", ""),
            "endpoint": "https://api.mexc.com/api/v3/klines",
            "params_builder": lambda s, i, l: {"symbol": s, "interval": i, "limit": l}
        },
        {
            "name": "Kraken",
            "priority": 7,
            "weight": 0.7,
            "symbol_fmt": lambda s: s.replace("/", "").replace("USDT", "USD"),
            "endpoint": "https://api.kraken.com/0/public/OHLC",
            "params_builder": lambda s, i, l: {"pair": s, "interval": i}
        },
        {
            "name": "Bitfinex",
            "priority": 8,
            "weight": 0.65,
            "symbol_fmt": lambda s: f"t{s.replace('/', '').replace('USDT', 'UST')}",
            "endpoint": "https://api-pub.bitfinex.com/v2/candles/trade:{interval}:{symbol}/hist",
            "endpoint_builder": True,
            "params_builder": lambda s, i, l: {"limit": l}
        },
        # Tier 4 - Fallback
        {
            "name": "Huobi",
            "priority": 9,
            "weight": 0.6,
            "symbol_fmt": lambda s: s.replace("/", "").lower(),
            "endpoint": "https://api.huobi.pro/market/history/kline",
            "params_builder": lambda s, i, l: {"symbol": s, "period": i, "size": l}
        },
        {
            "name": "Coinbase",
            "priority": 10,
            "weight": 0.55,
            "symbol_fmt": lambda s: s.replace("/", "-"),
            "endpoint": "https://api.exchange.coinbase.com/products/{symbol}/candles",
            "endpoint_builder": True,
            "params_builder": lambda s, i, l: {"granularity": i}
        },
        {
            "name": "Bitget",
            "priority": 11,
            "weight": 0.5,
            "symbol_fmt": lambda s: s.replace("/", ""),
            "endpoint": "https://api.bitget.com/api/spot/v1/market/candles",
            "params_builder": lambda s, i, l: {"symbol": s, "period": i, "limit": l}
        }
    ]
    
    INTERVAL_MAPPING = {
        "1m": {"Binance": "1m", "Bybit": "1", "OKX": "1m", "KuCoin": "1min", "Gate.io": "1m", 
               "MEXC": "1m", "Kraken": "1", "Bitfinex": "1m", "Huobi": "1min", "Coinbase": "60", "Bitget": "1m"},
        "5m": {"Binance": "5m", "Bybit": "5", "OKX": "5m", "KuCoin": "5min", "Gate.io": "5m", 
               "MEXC": "5m", "Kraken": "5", "Bitfinex": "5m", "Huobi": "5min", "Coinbase": "300", "Bitget": "5m"},
        "15m": {"Binance": "15m", "Bybit": "15", "OKX": "15m", "KuCoin": "15min", "Gate.io": "15m", 
                "MEXC": "15m", "Kraken": "15", "Bitfinex": "15m", "Huobi": "15min", "Coinbase": "900", "Bitget": "15m"},
        "30m": {"Binance": "30m", "Bybit": "30", "OKX": "30m", "KuCoin": "30min", "Gate.io": "30m", 
                "MEXC": "30m", "Kraken": "30", "Bitfinex": "30m", "Huobi": "30min", "Coinbase": "1800", "Bitget": "30m"},
        "1h": {"Binance": "1h", "Bybit": "60", "OKX": "1H", "KuCoin": "1hour", "Gate.io": "1h", 
               "MEXC": "1h", "Kraken": "60", "Bitfinex": "1h", "Huobi": "60min", "Coinbase": "3600", "Bitget": "1h"},
        "4h": {"Binance": "4h", "Bybit": "240", "OKX": "4H", "KuCoin": "4hour", "Gate.io": "4h", 
               "MEXC": "4h", "Kraken": "240", "Bitfinex": "4h", "Huobi": "4hour", "Coinbase": "14400", "Bitget": "4h"},
        "1d": {"Binance": "1d", "Bybit": "D", "OKX": "1D", "KuCoin": "1day", "Gate.io": "1d", 
               "MEXC": "1d", "Kraken": "1440", "Bitfinex": "1D", "Huobi": "1day", "Coinbase": "86400", "Bitget": "1d"},
        "1w": {"Binance": "1w", "Bybit": "W", "OKX": "1W", "KuCoin": "1week", "Gate.io": "1w", 
               "MEXC": "1w", "Kraken": "10080", "Bitfinex": "1W", "Huobi": "1week", "Coinbase": "604800", "Bitget": "1w"},
        "1M": {"Binance": "1M", "Bybit": "M", "OKX": "1M", "KuCoin": "1month", "Gate.io": "1M", 
               "MEXC": "1M", "Kraken": "43200", "Bitfinex": "1M", "Huobi": "1mon", "Coinbase": "2592000", "Bitget": "1M"}
    }
    
    def __init__(self):
        self.data_cache: Dict[str, List[Dict]] = {}
        self.cache_timestamps: Dict[str, float] = {}
        self.exchange_stats = defaultdict(lambda: {"success": 0, "fail": 0, "last_success": 0.0})
        self.session: Optional[aiohttp.ClientSession] = None
        logger.info(f"‚úÖ PriceFetcher initialized with {len(self.EXCHANGES)} exchanges")
    
    async def __aenter__(self):
        """Async context manager entry"""
        timeout = ClientTimeout(total=Config.EXCHANGE_TIMEOUT)
        connector = TCPConnector(limit=50, limit_per_host=10)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                "User-Agent": "TradingBot/v5.0",
                "Accept": "application/json"
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    def _get_exchange_interval(self, exchange_name: str, interval: str) -> Optional[str]:
        """Get exchange-specific interval"""
        if interval not in self.INTERVAL_MAPPING:
            return None
        return self.INTERVAL_MAPPING[interval].get(exchange_name)
    
    def _get_cache_key(self, symbol: str, interval: str) -> str:
        """Generate cache key"""
        return f"{symbol.upper()}_{interval}"
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """Check if cache is still valid"""
        if cache_key not in self.cache_timestamps:
            return False
        age = time.time() - self.cache_timestamps[cache_key]
        return age < Config.CACHE_TTL_SECONDS
    
    async def _fetch_single_exchange(
        self,
        exchange: Dict,
        symbol: str,
        interval: str,
        limit: int
    ) -> Optional[List[Dict]]:
        """Fetch data from a single exchange"""
        exchange_name = exchange["name"]
        
        # Circuit breaker check
        if not circuit_breaker.allow_request(exchange_name):
            logger.debug(f"üö´ Circuit breaker blocked {exchange_name}")
            return None
        
        try:
            # Get exchange-specific interval
            exchange_interval = self._get_exchange_interval(exchange_name, interval)
            if not exchange_interval:
                logger.debug(f"‚è≠Ô∏è {exchange_name} doesn't support interval {interval}")
                return None
            
            # Format symbol
            formatted_symbol = exchange["symbol_fmt"](symbol)
            
            # Build endpoint
            if exchange.get("endpoint_builder", False):
                endpoint = exchange["endpoint"].format(
                    symbol=formatted_symbol,
                    interval=exchange_interval
                )
            else:
                endpoint = exchange["endpoint"]
            
            # Build parameters
            params = exchange["params_builder"](formatted_symbol, exchange_interval, limit)
            
            # Make request
            start_time = time.time()
            async with self.session.get(endpoint, params=params) as response:
                elapsed = time.time() - start_time
                
                if response.status != 200:
                    circuit_breaker.record_failure(exchange_name)
                    self.exchange_stats[exchange_name]["fail"] += 1
                    logger.warning(f"‚ö†Ô∏è {exchange_name} HTTP {response.status} for {symbol}")
                    return None
                
                # Parse response
                data = await response.json()
                candles = self._parse_exchange_data(exchange_name, data)
                
                if not candles or len(candles) < 5:
                    circuit_breaker.record_failure(exchange_name)
                    self.exchange_stats[exchange_name]["fail"] += 1
                    logger.warning(f"‚ö†Ô∏è {exchange_name} insufficient data: {len(candles)} candles")
                    return None
                
                # Record success
                circuit_breaker.record_success(exchange_name)
                self.exchange_stats[exchange_name]["success"] += 1
                self.exchange_stats[exchange_name]["last_success"] = time.time()
                
                logger.debug(f"‚úÖ {exchange_name}: {len(candles)} candles in {elapsed:.2f}s")
                return candles
                
        except asyncio.TimeoutError:
            circuit_breaker.record_failure(exchange_name)
            self.exchange_stats[exchange_name]["fail"] += 1
            logger.warning(f"‚è±Ô∏è {exchange_name} timeout")
            return None
        except Exception as e:
            circuit_breaker.record_failure(exchange_name)
            self.exchange_stats[exchange_name]["fail"] += 1
            logger.error(f"‚ùå {exchange_name} error: {str(e)[:100]}")
            return None
    
    def _parse_exchange_data(self, exchange_name: str, data: Any) -> List[Dict]:
        """Parse exchange response to standard format"""
        candles = []
        
        try:
            if exchange_name == "Binance":
                for row in data:
                    candles.append({
                        "timestamp": int(row[0]),
                        "open": float(row[1]),
                        "high": float(row[2]),
                        "low": float(row[3]),
                        "close": float(row[4]),
                        "volume": float(row[5])
                    })
            
            elif exchange_name == "Bybit":
                if data.get("result") and data["result"].get("list"):
                    for row in data["result"]["list"]:
                        candles.append({
                            "timestamp": int(row[0]),
                            "open": float(row[1]),
                            "high": float(row[2]),
                            "low": float(row[3]),
                            "close": float(row[4]),
                            "volume": float(row[5])
                        })
            
            elif exchange_name == "OKX":
                if data.get("data"):
                    for row in data["data"]:
                        candles.append({
                            "timestamp": int(row[0]),
                            "open": float(row[1]),
                            "high": float(row[2]),
                            "low": float(row[3]),
                            "close": float(row[4]),
                            "volume": float(row[5])
                        })
            
            elif exchange_name == "KuCoin":
                if data.get("data"):
                    for row in data["data"]:
                        candles.append({
                            "timestamp": int(row[0]),
                            "open": float(row[1]),
                            "close": float(row[2]),
                            "high": float(row[3]),
                            "low": float(row[4]),
                            "volume": float(row[5])
                        })
            
            elif exchange_name == "Gate.io":
                if isinstance(data, list):
                    for row in data:
                        candles.append({
                            "timestamp": int(row[0]),
                            "volume": float(row[1]),
                            "close": float(row[2]),
                            "high": float(row[3]),
                            "low": float(row[4]),
                            "open": float(row[5])
                        })
            
            elif exchange_name == "MEXC":
                for row in data:
                    candles.append({
                        "timestamp": int(row[0]),
                        "open": float(row[1]),
                        "high": float(row[2]),
                        "low": float(row[3]),
                        "close": float(row[4]),
                        "volume": float(row[5])
                    })
            
            elif exchange_name == "Kraken":
                if data.get("result"):
                    first_key = next(iter(data["result"]))
                    for row in data["result"][first_key]:
                        candles.append({
                            "timestamp": int(float(row[0]) * 1000),
                            "open": float(row[1]),
                            "high": float(row[2]),
                            "low": float(row[3]),
                            "close": float(row[4]),
                            "volume": float(row[6])
                        })
            
            elif exchange_name == "Bitfinex":
                if isinstance(data, list):
                    for row in data:
                        candles.append({
                            "timestamp": int(row[0]),
                            "open": float(row[1]),
                            "close": float(row[2]),
                            "high": float(row[3]),
                            "low": float(row[4]),
                            "volume": float(row[5])
                        })
            
            elif exchange_name == "Huobi":
                if data.get("data"):
                    for row in data["data"]:
                        candles.append({
                            "timestamp": int(row["id"] * 1000),
                            "open": float(row["open"]),
                            "close": float(row["close"]),
                            "high": float(row["high"]),
                            "low": float(row["low"]),
                            "volume": float(row["vol"])
                        })
            
            elif exchange_name == "Coinbase":
                if isinstance(data, list):
                    for row in data:
                        candles.append({
                            "timestamp": int(row[0] * 1000),
                            "low": float(row[1]),
                            "high": float(row[2]),
                            "open": float(row[3]),
                            "close": float(row[4]),
                            "volume": float(row[5])
                        })
            
            elif exchange_name == "Bitget":
                if data.get("data"):
                    for row in data["data"]:
                        candles.append({
                            "timestamp": int(row[0]),
                            "open": float(row[1]),
                            "high": float(row[2]),
                            "low": float(row[3]),
                            "close": float(row[4]),
                            "volume": float(row[5])
                        })
            
            # Add exchange name to each candle
            for candle in candles:
                candle["exchange"] = exchange_name
            
            # Sort by timestamp
            candles.sort(key=lambda x: x["timestamp"])
            return candles
            
        except Exception as e:
            logger.error(f"‚ùå Parse error for {exchange_name}: {str(e)}")
            return []
    
    def _calculate_data_quality(self, candles: List[Dict]) -> float:
        """Calculate data quality score (0.0 to 1.0)"""
        if not candles:
            return 0.0
        
        # Check for gaps in timestamps
        timestamps = [c["timestamp"] for c in candles]
        if len(timestamps) < 2:
            return 0.5
        
        # Calculate time differences
        diffs = [timestamps[i] - timestamps[i-1] for i in range(1, len(timestamps))]
        avg_diff = sum(diffs) / len(diffs) if diffs else 0
        
        # Count gaps (more than 2x average difference)
        gaps = sum(1 for diff in diffs if diff > avg_diff * 2)
        gap_score = 1.0 - (gaps / len(diffs)) if diffs else 1.0
        
        # Check price validity
        valid_prices = 0
        total_prices = 0
        for candle in candles:
            if (candle["high"] >= candle["low"] and 
                candle["high"] >= max(candle["open"], candle["close"]) and
                candle["low"] <= min(candle["open"], candle["close"]) and
                all(v > 0 for v in [candle["open"], candle["high"], candle["low"], candle["close"]])):
                valid_prices += 1
            total_prices += 1
        
        price_score = valid_prices / total_prices if total_prices > 0 else 0.0
        
        # Overall quality score
        quality = (gap_score * 0.6) + (price_score * 0.4)
        return max(0.0, min(1.0, quality))
    
    def _aggregate_candles(self, all_candles: List[List[Dict]]) -> List[Dict]:
        """Aggregate candles from multiple sources"""
        if not all_candles:
            return []
        
        # Group by timestamp
        timestamp_map = defaultdict(list)
        for exchange_candles in all_candles:
            for candle in exchange_candles:
                timestamp_map[candle["timestamp"]].append(candle)
        
        # Aggregate
        aggregated = []
        for timestamp in sorted(timestamp_map.keys()):
            candles_at_ts = timestamp_map[timestamp]
            
            if len(candles_at_ts) == 1:
                aggregated.append(candles_at_ts[0])
            else:
                # Weighted average based on exchange priority
                weights = []
                opens, highs, lows, closes, volumes = [], [], [], [], []
                
                for candle in candles_at_ts:
                    exchange = next((e for e in self.EXCHANGES if e["name"] == candle["exchange"]), None)
                    weight = exchange["weight"] if exchange else 0.5
                    weights.append(weight)
                    opens.append(candle["open"] * weight)
                    highs.append(candle["high"] * weight)
                    lows.append(candle["low"] * weight)
                    closes.append(candle["close"] * weight)
                    volumes.append(candle["volume"] * weight)
                
                total_weight = sum(weights)
                if total_weight > 0:
                    aggregated.append({
                        "timestamp": timestamp,
                        "open": sum(opens) / total_weight,
                        "high": sum(highs) / total_weight,
                        "low": sum(lows) / total_weight,
                        "close": sum(closes) / total_weight,
                        "volume": sum(volumes) / total_weight,
                        "source_count": len(candles_at_ts),
                        "sources": [c["exchange"] for c in candles_at_ts],
                        "exchange": "aggregated"
                    })
        
        return aggregated
    
    async def get_candles(
        self,
        symbol: str,
        interval: str = "1h",
        limit: int = 100,
        force_refresh: bool = False
    ) -> List[Dict]:
        """Get candles with caching and aggregation"""
        cache_key = self._get_cache_key(symbol, interval)
        
        # Check cache
        if not force_refresh and self._is_cache_valid(cache_key):
            cached = self.data_cache.get(cache_key, [])
            if cached:
                logger.debug(f"üì¶ Cache hit for {symbol} {interval}")
                return cached[-limit:]
        
        # Fetch from exchanges
        logger.info(f"üîÑ Fetching {symbol} {interval} (limit: {limit})")
        
        tasks = []
        for exchange in self.EXCHANGES:
            task = self._fetch_single_exchange(exchange, symbol, interval, limit * 2)
            tasks.append(task)
        
        # Gather results
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter valid results
        valid_results = []
        for result in results:
            if isinstance(result, list) and len(result) >= 10:
                valid_results.append(result)
        
        # Check minimum sources requirement
        if len(valid_results) < Config.REQUIRE_MIN_EXCHANGES:
            logger.error(
                f"‚ùå Insufficient data sources: {len(valid_results)} "
                f"(min {Config.REQUIRE_MIN_EXCHANGES} required)"
            )
            return []
        
        # Aggregate results
        aggregated = self._aggregate_candles(valid_results)
        
        # Check minimum candles requirement
        if len(aggregated) < Config.MIN_CANDLES_FOR_ANALYSIS:
            logger.error(
                f"‚ùå Insufficient candles: {len(aggregated)} "
                f"(min {Config.MIN_CANDLES_FOR_ANALYSIS} required)"
            )
            return []
        
        # Update cache
        self.data_cache[cache_key] = aggregated
        self.cache_timestamps[cache_key] = time.time()
        
        # Trim cache if too large
        if len(self.data_cache) > Config.CACHE_MAX_SIZE:
            oldest_key = min(self.cache_timestamps.keys(), key=lambda k: self.cache_timestamps[k])
            del self.data_cache[oldest_key]
            del self.cache_timestamps[oldest_key]
        
        logger.info(f"‚úÖ Aggregated {len(aggregated)} candles from {len(valid_results)} sources")
        return aggregated[-limit:]
    
    def get_exchange_health(self) -> Dict[str, Any]:
        """Get health status of all exchanges"""
        now = time.time()
        health_report = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "total_exchanges": len(self.EXCHANGES),
            "healthy_exchanges": 0,
            "exchanges": {}
        }
        
        for exchange in self.EXCHANGES:
            name = exchange["name"]
            stats = self.exchange_stats[name]
            
            # Consider healthy if success in last 10 minutes
            is_healthy = (now - stats["last_success"]) < 600
            
            health_report["exchanges"][name] = {
                "priority": exchange["priority"],
                "success_count": stats["success"],
                "fail_count": stats["fail"],
                "last_success": datetime.fromtimestamp(stats["last_success"], tz=timezone.utc).isoformat() if stats["last_success"] > 0 else None,
                "is_healthy": is_healthy,
                "circuit_breaker": circuit_breaker.get_status(name)
            }
            
            if is_healthy:
                health_report["healthy_exchanges"] += 1
        
        health_report["health_score"] = round(
            health_report["healthy_exchanges"] / health_report["total_exchanges"] * 100, 
            1
        )
        
        return health_report

# ========== VOLATILITY TRAIL SYSTEM ==========
class VolatilityAdjustedTrail:
    """ICT-compliant volatility adjusted trail system"""
    
    @staticmethod
    def calculate_atr(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14) -> pd.Series:
        """Calculate Average True Range"""
        if len(high) < period + 1:
            return pd.Series([0.0] * len(high), index=high.index)
        
        tr1 = high - low
        tr2 = (high - close.shift(1)).abs()
        tr3 = (low - close.shift(1)).abs()
        
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        atr = tr.rolling(window=period).mean().fillna(tr.rolling(window=period, min_periods=1).mean())
        return atr.fillna(0)
    
    @staticmethod
    def calculate_trail(df: pd.DataFrame) -> Dict[str, Any]:
        """Calculate volatility adjusted trail"""
        if len(df) < 50:
            return {
                "current_trail": None,
                "direction": 0,
                "trend_strength": 0.0,
                "trail_type": "NEUTRAL",
                "atr_value": 0.0,
                "volatility_ratio": 1.0,
                "market_structure": "INSUFFICIENT_DATA"
            }
        
        try:
            # Calculate indicators
            close = df['close']
            high = df['high']
            low = df['low']
            
            atr = VolatilityAdjustedTrail.calculate_atr(high, low, close)
            ema_fast = close.ewm(span=9, adjust=False).mean()
            ema_slow = close.ewm(span=21, adjust=False).mean()
            
            # Volatility analysis
            volatility = close.rolling(window=20).std().fillna(0)
            avg_volatility = volatility.rolling(window=20).mean().fillna(volatility)
            vol_ratio = (volatility / avg_volatility).fillna(1.0).clip(0.5, 2.0)
            
            # Dynamic multiplier
            base_multiplier = 2.0
            dynamic_multiplier = base_multiplier * (1 + 0.3 * (vol_ratio - 1))
            
            # Calculate trails
            long_trail = high.rolling(window=20).max() - (atr * dynamic_multiplier)
            short_trail = low.rolling(window=20).min() + (atr * dynamic_multiplier)
            
            # Determine direction
            direction = 0
            if ema_fast.iloc[-1] > ema_slow.iloc[-1]:
                if ema_fast.iloc[-2] <= ema_slow.iloc[-2]:
                    direction = 1  # Bullish crossover
                else:
                    direction = 1  # Uptrend
            elif ema_fast.iloc[-1] < ema_slow.iloc[-1]:
                if ema_fast.iloc[-2] >= ema_slow.iloc[-2]:
                    direction = -1  # Bearish crossover
                else:
                    direction = -1  # Downtrend
            
            # Calculate trend strength
            if direction == 1:
                strength = (close.iloc[-1] - long_trail.iloc[-1]) / (atr.iloc[-1] + 1e-8)
            elif direction == -1:
                strength = (short_trail.iloc[-1] - close.iloc[-1]) / (atr.iloc[-1] + 1e-8)
            else:
                strength = 0.0
            
            trend_strength = float(np.clip(strength, 0.0, 1.0))
            
            # Determine current trail
            current_trail = None
            trail_type = "NEUTRAL"
            if direction == 1:
                current_trail = float(long_trail.iloc[-1])
                trail_type = "LONG"
            elif direction == -1:
                current_trail = float(short_trail.iloc[-1])
                trail_type = "SHORT"
            
            # Market structure
            if direction == 1:
                market_structure = "BULLISH"
            elif direction == -1:
                market_structure = "BEARISH"
            else:
                market_structure = "RANGING"
            
            return {
                "current_trail": current_trail,
                "direction": direction,
                "trend_strength": trend_strength,
                "trail_type": trail_type,
                "atr_value": float(atr.iloc[-1]),
                "volatility_ratio": float(vol_ratio.iloc[-1]),
                "dynamic_multiplier": float(dynamic_multiplier.iloc[-1]),
                "market_structure": market_structure,
                "ema_fast": float(ema_fast.iloc[-1]),
                "ema_slow": float(ema_slow.iloc[-1]),
                "current_price": float(close.iloc[-1])
            }
            
        except Exception as e:
            logger.error(f"‚ùå Trail calculation error: {str(e)}")
            return {
                "current_trail": None,
                "direction": 0,
                "trend_strength": 0.0,
                "trail_type": "ERROR",
                "atr_value": 0.0,
                "volatility_ratio": 1.0,
                "market_structure": "CALCULATION_ERROR"
            }

# ========== AI TRADING ENGINE ==========
class AITradingEngine:
    """AI trading engine with ensemble models"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.lgb_models = {}
        self.feature_columns = []
        logger.info("‚úÖ AI Trading Engine initialized")
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create comprehensive features for ML"""
        try:
            if len(df) < 30:
                return pd.DataFrame()
            
            df = df.copy()
            
            # Basic returns
            df['returns'] = df['close'].pct_change().fillna(0)
            df['log_returns'] = np.log(df['close'] / df['close'].shift(1)).fillna(0)
            
            # Moving averages
            for period in [5, 9, 20, 50]:
                df[f'sma_{period}'] = df['close'].rolling(period).mean().fillna(method='ffill')
                df[f'ema_{period}'] = df['close'].ewm(span=period, adjust=False).mean().fillna(method='ffill')
            
            # RSI
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss.replace(0, np.nan)
            df['rsi'] = (100 - (100 / (1 + rs))).fillna(50)
            
            # MACD
            exp1 = df['close'].ewm(span=12, adjust=False).mean()
            exp2 = df['close'].ewm(span=26, adjust=False).mean()
            df['macd'] = exp1 - exp2
            df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()
            df['macd_hist'] = df['macd'] - df['macd_signal']
            
            # Bollinger Bands
            df['bb_middle'] = df['close'].rolling(20).mean()
            bb_std = df['close'].rolling(20).std()
            df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
            df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
            df['bb_width'] = df['bb_upper'] - df['bb_lower']
            
            # Volume indicators
            df['volume_sma'] = df['volume'].rolling(20).mean()
            df['volume_ratio'] = df['volume'] / df['volume_sma'].replace(0, 1)
            
            # Price patterns
            df['body_size'] = (df['close'] - df['open']).abs() / df['open']
            df['high_low_range'] = (df['high'] - df['low']) / df['close']
            
            # Momentum
            df['momentum_5'] = df['close'].pct_change(5)
            df['momentum_10'] = df['close'].pct_change(10)
            
            # Drop NaN values
            df = df.dropna()
            
            # Store feature columns (exclude target and timestamp columns)
            non_feature_cols = ['timestamp', 'datetime', 'exchange', 'source_count', 'sources']
            self.feature_columns = [col for col in df.columns if col not in non_feature_cols]
            
            logger.info(f"‚úÖ Created {len(self.feature_columns)} features")
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Feature creation error: {str(e)}")
            return pd.DataFrame()
    
    def prepare_sequences(self, df: pd.DataFrame, sequence_length: int = 60) -> Tuple[np.ndarray, np.ndarray]:
        """Prepare sequences for time-series models"""
        if df.empty or len(df) < sequence_length:
            return np.array([]), np.array([])
        
        try:
            features = df[self.feature_columns].values.astype(np.float32)
            
            # Create sequences
            X, y = [], []
            for i in range(len(features) - sequence_length):
                X.append(features[i:i+sequence_length])
                # Simple binary target: will price go up in next period?
                future_return = features[i+sequence_length, df.columns.get_loc('close')] / features[i+sequence_length-1, df.columns.get_loc('close')] - 1
                y.append(1 if future_return > 0 else 0)
            
            return np.array(X), np.array(y)
            
        except Exception as e:
            logger.error(f"‚ùå Sequence preparation error: {str(e)}")
            return np.array([]), np.array([])
    
    async def train_lightgbm(self, symbol: str, df: pd.DataFrame) -> bool:
        """Train LightGBM model"""
        try:
            if not ML_AVAILABLE:
                return False
            
            # Create features
            df_features = self.create_features(df)
            if df_features.empty:
                return False
            
            # Prepare data
            features = df_features[self.feature_columns].values
            target = (df_features['close'].shift(-5) > df_features['close']).astype(int).values[:-5]
            features = features[:-5]
            
            if len(features) < 100:
                return False
            
            # Split data
            split_idx = int(len(features) * 0.8)
            X_train, X_val = features[:split_idx], features[split_idx:]
            y_train, y_val = target[:split_idx], target[split_idx:]
            
            # Train model
            params = {
                'objective': 'binary',
                'metric': 'binary_logloss',
                'boosting_type': 'gbdt',
                'num_leaves': 31,
                'learning_rate': 0.05,
                'feature_fraction': 0.9,
                'verbose': -1
            }
            
            train_data = lgb.Dataset(X_train, label=y_train)
            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
            
            model = lgb.train(
                params,
                train_data,
                valid_sets=[val_data],
                num_boost_round=100,
                callbacks=[lgb.log_evaluation(0)]
            )
            
            self.lgb_models[symbol] = model
            logger.info(f"‚úÖ LightGBM trained for {symbol}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå LightGBM training failed: {str(e)}")
            return False
    
    def predict(self, symbol: str, df: pd.DataFrame) -> Dict[str, Any]:
        """Generate prediction using available models"""
        try:
            if df.empty or len(df) < 30:
                return {
                    "prediction": "NEUTRAL",
                    "confidence": 0.5,
                    "method": "insufficient_data",
                    "details": {}
                }
            
            # Create features
            df_features = self.create_features(df)
            if df_features.empty:
                return {
                    "prediction": "NEUTRAL",
                    "confidence": 0.5,
                    "method": "feature_error",
                    "details": {}
                }
            
            predictions = []
            confidences = []
            details = {}
            
            # LightGBM prediction
            if symbol in self.lgb_models and self.feature_columns:
                try:
                    model = self.lgb_models[symbol]
                    recent_features = df_features[self.feature_columns].iloc[-1:].values
                    prob = model.predict(recent_features)[0]
                    
                    predictions.append("BUY" if prob > 0.5 else "SELL")
                    confidences.append(float(prob if prob > 0.5 else 1 - prob))
                    details["lightgbm"] = {
                        "probability": float(prob),
                        "prediction": "BUY" if prob > 0.5 else "SELL",
                        "confidence": float(prob if prob > 0.5 else 1 - prob)
                    }
                except Exception as e:
                    logger.warning(f"LightGBM prediction error: {e}")
            
            # Technical analysis fallback
            if not predictions:
                # Simple SMA crossover strategy
                sma_fast = df['close'].rolling(9).mean().iloc[-1]
                sma_slow = df['close'].rolling(21).mean().iloc[-1]
                
                if sma_fast > sma_slow:
                    predictions.append("BUY")
                    confidences.append(0.6)
                    details["technical"] = {
                        "method": "sma_crossover",
                        "sma_fast": float(sma_fast),
                        "sma_slow": float(sma_slow)
                    }
                else:
                    predictions.append("SELL")
                    confidences.append(0.6)
                    details["technical"] = {
                        "method": "sma_crossover",
                        "sma_fast": float(sma_fast),
                        "sma_slow": float(sma_slow)
                    }
            
            # Ensemble result
            if predictions:
                # Simple voting
                buy_count = predictions.count("BUY")
                sell_count = predictions.count("SELL")
                
                if buy_count > sell_count:
                    final_pred = "BUY"
                    avg_conf = sum(c for p, c in zip(predictions, confidences) if p == "BUY") / buy_count
                elif sell_count > buy_count:
                    final_pred = "SELL"
                    avg_conf = sum(c for p, c in zip(predictions, confidences) if p == "SELL") / sell_count
                else:
                    final_pred = "NEUTRAL"
                    avg_conf = 0.5
                
                # Upgrade to strong signals for high confidence
                if avg_conf > 0.75:
                    if final_pred == "BUY":
                        final_pred = "STRONG_BUY"
                    elif final_pred == "SELL":
                        final_pred = "STRONG_SELL"
                
                return {
                    "prediction": final_pred,
                    "confidence": float(avg_conf),
                    "method": "ensemble",
                    "model_count": len(predictions),
                    "details": details
                }
            else:
                return {
                    "prediction": "NEUTRAL",
                    "confidence": 0.5,
                    "method": "fallback",
                    "details": {}
                }
                
        except Exception as e:
            logger.error(f"‚ùå Prediction error: {str(e)}")
            return {
                "prediction": "NEUTRAL",
                "confidence": 0.3,
                "method": "error",
                "error": str(e)[:200],
                "details": {}
            }

# ========== GLOBAL INSTANCES ==========
price_fetcher = PriceFetcher()
ai_engine = AITradingEngine()
websocket_connections: Set[WebSocket] = set()
startup_time = time.time()

# ========== API ENDPOINTS ==========
@app.get("/", response_class=HTMLResponse)
async def root():
    """Root endpoint - redirects to dashboard"""
    return """
    <html>
        <head>
            <title>Trading Bot v5.0</title>
            <meta http-equiv="refresh" content="0;url=/dashboard">
            <style>
                body {
                    font-family: Arial, sans-serif;
                    display: flex;
                    justify-content: center;
                    align-items: center;
                    height: 100vh;
                    margin: 0;
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                }
                .container {
                    text-align: center;
                }
                h1 {
                    font-size: 3em;
                    margin-bottom: 20px;
                }
                p {
                    font-size: 1.2em;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>üöÄ Trading Bot v5.0</h1>
                <p>Enterprise-Grade Trading System</p>
                <p>Redirecting to dashboard...</p>
            </div>
        </body>
    </html>
    """

@app.get("/dashboard", response_class=HTMLResponse)
async def dashboard(request: Request):
    """Trading dashboard"""
    context = {
        "request": request,
        "title": "Trading Bot Dashboard",
        "version": "5.0.0",
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC"),
        "status": "üü¢ Active",
        "refresh_interval": Config.DASHBOARD_REFRESH_INTERVAL
    }
    return templates.TemplateResponse("dashboard.html", context)

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        # Get exchange health
        async with price_fetcher as fetcher:
            exchange_health = fetcher.get_exchange_health()
        
        # Calculate uptime
        uptime = time.time() - startup_time
        
        # Determine overall status
        health_score = exchange_health["health_score"]
        if health_score >= 80:
            status = "healthy"
        elif health_score >= 50:
            status = "degraded"
        else:
            status = "unhealthy"
        
        return {
            "status": status,
            "version": "5.0.0",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "uptime_seconds": int(uptime),
            "uptime_human": str(timedelta(seconds=int(uptime))),
            "exchange_health": exchange_health,
            "ml_status": {
                "available": ML_AVAILABLE,
                "models_trained": len(ai_engine.lgb_models)
            },
            "cache_status": {
                "size": len(price_fetcher.data_cache),
                "max_size": Config.CACHE_MAX_SIZE
            }
        }
        
    except Exception as e:
        logger.error(f"Health check error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Health check failed: {str(e)}")

@app.get("/api/analyze/{symbol}")
async def analyze_symbol(
    symbol: str,
    interval: str = Query(default="1h", regex="^(1m|5m|15m|30m|1h|4h|1d|1w|1M)$"),
    limit: int = Query(default=100, ge=10, le=1000)
):
    """Analyze a trading symbol"""
    symbol = symbol.upper()
    if not symbol.endswith("USDT"):
        symbol = f"{symbol}USDT"
    
    logger.info(f"üîç Analyzing {symbol} ({interval})")
    
    try:
        # Fetch candles
        async with price_fetcher as fetcher:
            candles = await fetcher.get_candles(symbol, interval, limit)
        
        # Check if we have enough data
        if not candles or len(candles) < Config.MIN_CANDLES_FOR_ANALYSIS:
            raise HTTPException(
                status_code=422,
                detail=f"Insufficient real market data for {symbol}. "
                       f"Got {len(candles) if candles else 0} candles, "
                       f"need at least {Config.MIN_CANDLES_FOR_ANALYSIS}."
            )
        
        # Convert to DataFrame
        df = pd.DataFrame(candles)
        if 'timestamp' not in df.columns:
            raise HTTPException(status_code=500, detail="Invalid candle data format")
        
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        
        # Calculate volatility trail
        trail_result = VolatilityAdjustedTrail.calculate_trail(df)
        
        # Get ML prediction
        ml_prediction = ai_engine.predict(symbol, df)
        
        # Calculate price change
        current_price = df['close'].iloc[-1]
        prev_price = df['close'].iloc[-2] if len(df) > 1 else current_price
        price_change_pct = ((current_price - prev_price) / prev_price * 100) if prev_price != 0 else 0
        
        # Calculate data quality
        quality_score = price_fetcher._calculate_data_quality(candles)
        
        response = {
            "success": True,
            "symbol": symbol,
            "interval": interval,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "price_data": {
                "current": float(current_price),
                "previous": float(prev_price),
                "change_percent": round(price_change_pct, 4),
                "high_24h": float(df['high'].max()),
                "low_24h": float(df['low'].min()),
                "volume_24h": float(df['volume'].sum())
            },
            "volatility_trail": trail_result,
            "ml_prediction": ml_prediction,
            "data_quality": {
                "candle_count": len(candles),
                "quality_score": round(quality_score, 4),
                "source_count": candles[0].get('source_count', 1) if candles else 0,
                "sources": candles[0].get('sources', []) if candles else []
            },
            "market_context": {
                "market_structure": trail_result.get("market_structure", "UNKNOWN"),
                "trend_strength": trail_result.get("trend_strength", 0.0),
                "volatility_regime": "HIGH" if trail_result.get("volatility_ratio", 1.0) > 1.5 
                                     else "LOW" if trail_result.get("volatility_ratio", 1.0) < 0.7 
                                     else "NORMAL"
            }
        }
        
        logger.info(f"‚úÖ Analysis complete for {symbol}: {ml_prediction['prediction']} "
                   f"({ml_prediction['confidence']:.2%})")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Analysis failed for {symbol}: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)[:200]}")

@app.post("/api/train/{symbol}")
async def train_model(symbol: str):
    """Train ML model for a symbol"""
    symbol = symbol.upper()
    if not symbol.endswith("USDT"):
        symbol = f"{symbol}USDT"
    
    logger.info(f"üß† Training model for {symbol}")
    
    try:
        # Fetch training data
        async with price_fetcher as fetcher:
            candles = await fetcher.get_candles(symbol, "1h", 1000, force_refresh=True)
        
        if not candles or len(candles) < Config.MIN_CANDLES_FOR_TRAINING:
            raise HTTPException(
                status_code=400,
                detail=f"Insufficient data for training. "
                       f"Got {len(candles) if candles else 0} candles, "
                       f"need at least {Config.MIN_CANDLES_FOR_TRAINING}."
            )
        
        # Convert to DataFrame
        df = pd.DataFrame(candles)
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        
        # Train model
        success = await ai_engine.train_lightgbm(symbol, df)
        
        if success:
            return {
                "success": True,
                "message": f"Model trained successfully for {symbol}",
                "symbol": symbol,
                "candles_used": len(candles),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        else:
            raise HTTPException(
                status_code=500,
                detail="Model training failed. Check logs for details."
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Training failed for {symbol}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Training failed: {str(e)[:200]}")

@app.websocket("/ws/{symbol}")
async def websocket_endpoint(websocket: WebSocket, symbol: str):
    """WebSocket for real-time updates"""
    symbol = symbol.upper()
    if not symbol.endswith("USDT"):
        symbol = f"{symbol}USDT"
    
    await websocket.accept()
    websocket_connections.add(websocket)
    logger.info(f"üîó WebSocket connected for {symbol} (total: {len(websocket_connections)})")
    
    try:
        last_price = None
        
        while True:
            # Fetch latest data
            async with price_fetcher as fetcher:
                candles = await fetcher.get_candles(symbol, "1m", 2)
            
            if candles and len(candles) >= 2:
                current_price = candles[-1]['close']
                prev_price = candles[-2]['close']
                
                # Only send update if price changed
                if last_price is None or current_price != last_price:
                    change_pct = ((current_price - prev_price) / prev_price * 100) if prev_price != 0 else 0
                    
                    message = {
                        "type": "price_update",
                        "symbol": symbol,
                        "price": float(current_price),
                        "change_percent": round(change_pct, 4),
                        "volume": float(candles[-1]['volume']),
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    }
                    
                    await websocket.send_json(message)
                    last_price = current_price
            
            # Heartbeat
            await websocket.send_json({
                "type": "heartbeat",
                "timestamp": datetime.now(timezone.utc).isoformat()
            })
            
            # Wait before next update
            await asyncio.sleep(1)
            
    except WebSocketDisconnect:
        logger.info(f"‚ùå WebSocket disconnected for {symbol}")
    except Exception as e:
        logger.error(f"WebSocket error for {symbol}: {str(e)}")
    finally:
        websocket_connections.discard(websocket)

@app.get("/api/exchanges")
async def get_exchanges():
    """Get exchange health and status"""
    async with price_fetcher as fetcher:
        health = fetcher.get_exchange_health()
    
    return {
        "success": True,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "data": health
    }

# ========== STARTUP AND SHUTDOWN ==========
@app.on_event("startup")
async def startup_event():
    """Initialize application on startup"""
    # Initialize rate limiting
    try:
        redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
        redis_client = redis.from_url(redis_url, encoding="utf-8", decode_responses=True)
        await FastAPILimiter.init(redis_client)
        logger.info("‚úÖ Rate limiting initialized")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Rate limiting disabled: {str(e)}")
    
    logger.info("="*80)
    logger.info("üöÄ TRADING BOT v5.0 STARTED SUCCESSFULLY")
    logger.info("="*80)
    logger.info(f"Environment: {Config.ENV}")
    logger.info(f"Debug Mode: {Config.DEBUG}")
    logger.info(f"Real Data Only: {'ENFORCED' if not Config.ALLOW_SYNTHETIC_DATA else 'WARNING: DISABLED'}")
    logger.info(f"Minimum Exchanges: {Config.REQUIRE_MIN_EXCHANGES}")
    logger.info(f"ML Available: {ML_AVAILABLE}")
    logger.info("="*80)

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    logger.info("="*80)
    logger.info("üõë SHUTTING DOWN TRADING BOT")
    logger.info("="*80)
    
    # Close WebSocket connections
    for ws in list(websocket_connections):
        try:
            await ws.close()
        except:
            pass
    
    logger.info(f"‚úÖ Cleanup complete. Uptime: {time.time() - startup_time:.0f} seconds")
    logger.info("="*80)

# ========== MAIN ENTRY POINT ==========
if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")
    
    logger.info(f"üåê Starting server on {host}:{port}")
    
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=Config.DEBUG,
        log_level="info",
        access_log=False
    )
