"""
üöÄ PROFESSIONAL TRADING BOT v5.0 - ENTERPRISE GRADE BACKEND
‚úÖ 11+ Exchange Support (Binance, Bybit, OKX, KuCoin, Gate.io, MEXC, Kraken, Bitfinex, Huobi, Coinbase, Bitget)
‚úÖ Real Data Only - ZERO Synthetic/Fake Data Fallback
‚úÖ Volatility Adjusted Trail System (ICT Compliant)
‚úÖ Advanced ML Ensemble (LSTM + Transformer + LightGBM)
‚úÖ 12+ Candlestick Patterns + ICT Market Structure
‚úÖ Circuit Breaker Pattern for Exchange Failures
‚úÖ Smart Cache with TTL & Source Validation
‚úÖ WebSocket Real-Time Streaming
‚úÖ Enterprise Logging & Monitoring
‚úÖ Rate Limiting & Security Hardening
‚úÖ Data Quality Validation Pipeline
"""
import os
import logging
import sys
from logging.handlers import RotatingFileHandler
from datetime import datetime, timedelta, timezone
import json
import asyncio
import time
import random
from typing import Dict, List, Optional, Tuple, Any, Set
import aiohttp
from aiohttp import ClientTimeout, TCPConnector
from enum import Enum
from collections import defaultdict, deque
import hashlib
import secrets

# Machine Learning Libraries (Lazy Import for Faster Startup)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    from torch.utils.data import TensorDataset, DataLoader
    import numpy as np
    import pandas as pd
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import accuracy_score
    import lightgbm as lgb
    ML_AVAILABLE = True
except ImportError as e:
    logging.warning(f"ML libraries not available: {e}")
    ML_AVAILABLE = False

# FastAPI & Security
from fastapi import FastAPI, Request, HTTPException, Query, WebSocket, WebSocketDisconnect, status
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.security import HTTPBearer
from fastapi_limiter import FastAPILimiter
from fastapi_limiter.depends import RateLimiter
import redis.asyncio as redis

# ========== ENTERPRISE LOGGING CONFIGURATION ==========
def setup_logging():
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    
    # Main logger
    logger = logging.getLogger("trading_bot")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter(
        '%(asctime)s | %(levelname)-8s | %(name)-15s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    
    # Rotating file handler (10MB x 5 files)
    file_handler = RotatingFileHandler(
        os.path.join(log_dir, 'trading_bot.log'),
        maxBytes=10*1024*1024,
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter(
        '%(asctime)s | %(levelname)-8s | %(name)-15s | %(funcName)-20s | %(lineno)4d | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)
    
    # Error-specific handler
    error_handler = RotatingFileHandler(
        os.path.join(log_dir, 'errors.log'),
        maxBytes=10*1024*1024,
        backupCount=5,
        encoding='utf-8'
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(file_formatter)
    logger.addHandler(error_handler)
    
    # Disable noisy libraries
    logging.getLogger("aiohttp").setLevel(logging.WARNING)
    logging.getLogger("asyncio").setLevel(logging.WARNING)
    logging.getLogger("lightgbm").setLevel(logging.WARNING)
    
    return logger

logger = setup_logging()
logger.info("="*80)
logger.info("üöÄ TRADING BOT BACKEND v5.0 STARTING - ENTERPRISE EDITION")
logger.info("="*80)

# ========== SECURITY & CONFIGURATION ==========
class Config:
    # Security
    API_KEY = os.getenv("TRADING_BOT_API_KEY", secrets.token_urlsafe(32))
    SECRET_KEY = os.getenv("SECRET_KEY", secrets.token_urlsafe(64))
    
    # Rate Limiting
    RATE_LIMIT_PER_MINUTE = 60
    RATE_LIMIT_PER_HOUR = 1000
    
    # Cache
    CACHE_TTL_SECONDS = 60  # 1 minute for price data
    CACHE_MAX_SIZE = 1000
    
    # Exchange Settings
    EXCHANGE_TIMEOUT = 10  # seconds
    EXCHANGE_MAX_RETRIES = 2
    CIRCUIT_BREAKER_FAILURE_THRESHOLD = 5
    CIRCUIT_BREAKER_TIMEOUT = 30  # seconds
    
    # Data Validation
    MIN_CANDLES_FOR_ANALYSIS = 50
    MIN_CANDLES_FOR_TRAINING = 500
    MIN_DATA_QUALITY_SCORE = 0.7  # 70% data completeness
    
    # ML Settings
    ML_SEQUENCE_LENGTH = 60
    ML_FEATURES_MIN = 30
    
    # Production Safety
    ALLOW_SYNTHETIC_DATA = False  # HARD POLICY: NEVER ALLOW SYNTHETIC DATA
    REQUIRE_MIN_EXCHANGES = 2  # Require data from at least 2 exchanges
    
    # Environment
    ENV = os.getenv("ENV", "production")
    DEBUG = os.getenv("DEBUG", "false").lower() == "true"
    
    @classmethod
    def validate(cls):
        if cls.ENV == "production" and cls.ALLOW_SYNTHETIC_DATA:
            raise ValueError("SYNTHETIC DATA NOT ALLOWED IN PRODUCTION")
        logger.info(f"‚úÖ Configuration validated for {cls.ENV} environment")

Config.validate()

# ========== FASTAPI APPLICATION ==========
app = FastAPI(
    title="Professional AI Trading Bot v5.0",
    description="Enterprise-grade trading system with real data only policy",
    version="5.0.0",
    docs_url="/docs" if Config.DEBUG else None,
    redoc_url="/redoc" if Config.DEBUG else None,
    openapi_url="/openapi.json" if Config.DEBUG else None,
    contact={
        "name": "Trading Bot Support",
        "email": "support@tradingbot.com"
    },
    license_info={
        "name": "Proprietary",
        "url": "https://tradingbot.com/license"
    }
)

# Security Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"] if Config.DEBUG else ["https://yourdomain.com", "https://tradingbot.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["X-Request-ID", "X-RateLimit-Limit", "X-RateLimit-Remaining"],
)

app.add_middleware(GZipMiddleware, minimum_size=1000)

# Rate Limiting Setup
@app.on_event("startup")
async def startup():
    redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
    try:
        r = redis.from_url(redis_url, encoding="utf-8", decode_responses=True)
        await FastAPILimiter.init(r)
        logger.info(f"‚úÖ Redis rate limiting initialized: {redis_url}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Redis not available, using in-memory rate limiting: {e}")
        # Fallback to in-memory rate limiting

# Security Headers Middleware
@app.middleware("http")
async def add_security_headers(request: Request, call_next):
    response = await call_next(request)
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
    response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline' https://cdn.jsdelivr.net https://s3.tradingview.com; style-src 'self' 'unsafe-inline' https://cdn.jsdelivr.net https://fonts.googleapis.com; font-src 'self' https://fonts.gstatic.com; img-src 'self' data: https://*; connect-src 'self' wss://* https://*"
    response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    return response

# ========== CORE ENUMERATIONS ==========
class SignalType(str, Enum):
    STRONG_BUY = "STRONG_BUY"
    BUY = "BUY"
    NEUTRAL = "NEUTRAL"
    SELL = "SELL"
    STRONG_SELL = "STRONG_SELL"

class Timeframe(str, Enum):
    _1M = "1m"
    _5M = "5m"
    _15M = "15m"
    _30M = "30m"
    _1H = "1h"
    _4H = "4h"
    _1D = "1d"
    _1W = "1w"
    _1MON = "1M"

# ========== CIRCUIT BREAKER FOR EXCHANGES ==========
class CircuitBreakerState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class ExchangeCircuitBreaker:
    """Prevent cascading failures by isolating failing exchanges"""
    def __init__(self, failure_threshold: int = 5, timeout: int = 30):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failures: Dict[str, int] = defaultdict(int)
        self.state: Dict[str, CircuitBreakerState] = defaultdict(lambda: CircuitBreakerState.CLOSED)
        self.last_failure_time: Dict[str, float] = {}
    
    def allow_request(self, exchange_name: str) -> bool:
        current_state = self.state[exchange_name]
        
        if current_state == CircuitBreakerState.CLOSED:
            return True
        
        if current_state == CircuitBreakerState.OPEN:
            # Check if timeout has passed to try again
            if time.time() - self.last_failure_time.get(exchange_name, 0) > self.timeout:
                self.state[exchange_name] = CircuitBreakerState.HALF_OPEN
                logger.info(f"üîÑ Circuit breaker half-open for {exchange_name}")
                return True
            return False
        
        # HALF_OPEN state - allow one request to test
        return True
    
    def record_success(self, exchange_name: str):
        self.failures[exchange_name] = 0
        self.state[exchange_name] = CircuitBreakerState.CLOSED
        logger.debug(f"‚úÖ Circuit breaker closed for {exchange_name} after success")
    
    def record_failure(self, exchange_name: str):
        self.failures[exchange_name] += 1
        self.last_failure_time[exchange_name] = time.time()
        
        if self.failures[exchange_name] >= self.failure_threshold:
            self.state[exchange_name] = CircuitBreakerState.OPEN
            logger.warning(f"‚ö†Ô∏è Circuit breaker OPENED for {exchange_name} after {self.failures[exchange_name]} failures")

circuit_breaker = ExchangeCircuitBreaker(
    failure_threshold=Config.CIRCUIT_BREAKER_FAILURE_THRESHOLD,
    timeout=Config.CIRCUIT_BREAKER_TIMEOUT
)

# ========== PROFESSIONAL PRICE FETCHER (11+ EXCHANGES) ==========
class PriceFetcher:
    """
    Enterprise-Grade Multi-Exchange Data Fetcher
    - 11+ Cryptocurrency Exchanges
    - Real Data Only Policy (NO Synthetic Data)
    - Circuit Breaker Pattern
    - Smart Aggregation with Source Validation
    - Data Quality Scoring
    - Smart Caching with TTL
    """
    
    # Complete Exchange Configuration (11+ Exchanges)
    EXCHANGES = [
        # Tier 1 - Highest Priority
        {
            "name": "Binance",
            "priority": 1,
            "type": "crypto",
            "symbol_fmt": lambda s: s,
            "endpoint": "https://api.binance.com/api/v3/klines",
            "method": "GET",
            "params_builder": lambda s, i, l: {"symbol": s, "interval": i, "limit": l},
            "weight": 1.0
        },
        {
            "name": "Bybit",
            "priority": 2,
            "type": "crypto",
            "symbol_fmt": lambda s: s,
            "endpoint": "https://api.bybit.com/v5/market/kline",
            "method": "GET",
            "params_builder": lambda s, i, l: {"category": "spot", "symbol": s, "interval": i, "limit": l},
            "weight": 0.95
        },
        {
            "name": "OKX",
            "priority": 3,
            "type": "crypto",
            "symbol_fmt": lambda s: s.replace("USDT", "-USDT"),
            "endpoint": "https://www.okx.com/api/v5/market/candles",
            "method": "GET",
            "params_builder": lambda s, i, l: {"instId": s, "bar": i, "limit": str(l)},
            "weight": 0.9
        },
        # Tier 2 - High Priority
        {
            "name": "KuCoin",
            "priority": 4,
            "type": "crypto",
            "symbol_fmt": lambda s: s.replace("USDT", "-USDT"),
            "endpoint": "https://api.kucoin.com/api/v1/market/candles",
            "method": "GET",
            "params_builder": lambda s, i, l: {"symbol": s, "type": i},
            "weight": 0.85
        },
        {
            "name": "Gate.io",
            "priority": 5,
            "type": "crypto",
            "symbol_fmt": lambda s: s.replace("USDT", "_USDT"),
            "endpoint": "https://api.gateio.ws/api/v4/spot/candlesticks",
            "method": "GET",
            "params_builder": lambda s, i, l: {"currency_pair": s, "interval": i, "limit": l},
            "weight": 0.8
        },
        {
            "name": "MEXC",
            "priority": 6,
            "type": "crypto",
            "symbol_fmt": lambda s: s,
            "endpoint": "https://api.mexc.com/api/v3/klines",
            "method": "GET",
            "params_builder": lambda s, i, l: {"symbol": s, "interval": i, "limit": l},
            "weight": 0.75
        },
        # Tier 3 - Medium Priority
        {
            "name": "Kraken",
            "priority": 7,
            "type": "crypto",
            "symbol_fmt": lambda s: s.replace("USDT", "USD"),
            "endpoint": "https://api.kraken.com/0/public/OHLC",
            "method": "GET",
            "params_builder": lambda s, i, l: {"pair": s, "interval": i},
            "weight": 0.7
        },
        {
            "name": "Bitfinex",
            "priority": 8,
            "type": "crypto",
            "symbol_fmt": lambda s: f"t{s.replace('USDT', 'UST')}",
            "endpoint_builder": lambda s, i: f"https://api-pub.bitfinex.com/v2/candles/trade:{i}:{s}/hist",
            "method": "GET",
            "params_builder": lambda s, i, l: {"limit": l},
            "weight": 0.65
        },
        {
            "name": "Huobi",
            "priority": 9,
            "type": "crypto",
            "symbol_fmt": lambda s: s.lower(),
            "endpoint": "https://api.huobi.pro/market/history/kline",
            "method": "GET",
            "params_builder": lambda s, i, l: {"symbol": s, "period": i, "size": l},
            "weight": 0.6
        },
        # Tier 4 - Lower Priority (Fallback)
        {
            "name": "Coinbase",
            "priority": 10,
            "type": "crypto",
            "symbol_fmt": lambda s: s.replace("USDT", "-USD"),
            "endpoint_builder": lambda s, i: f"https://api.exchange.coinbase.com/products/{s}/candles",
            "method": "GET",
            "params_builder": lambda s, i, l: {"granularity": i},
            "weight": 0.55
        },
        {
            "name": "Bitget",
            "priority": 11,
            "type": "crypto",
            "symbol_fmt": lambda s: s,
            "endpoint": "https://api.bitget.com/api/spot/v1/market/candles",
            "method": "GET",
            "params_builder": lambda s, i, l: {"symbol": s, "period": i, "limit": l},
            "weight": 0.5
        }
    ]
    
    # Complete Interval Mapping for ALL Timeframes & Exchanges
    INTERVAL_MAPPING = {
        "1m": {
            "Binance": "1m", "Bybit": "1", "OKX": "1m", "KuCoin": "1min", "Gate.io": "1m",
            "MEXC": "1m", "Kraken": "1", "Bitfinex": "1m", "Huobi": "1min", "Coinbase": "60", "Bitget": "1m"
        },
        "5m": {
            "Binance": "5m", "Bybit": "5", "OKX": "5m", "KuCoin": "5min", "Gate.io": "5m",
            "MEXC": "5m", "Kraken": "5", "Bitfinex": "5m", "Huobi": "5min", "Coinbase": "300", "Bitget": "5m"
        },
        "15m": {
            "Binance": "15m", "Bybit": "15", "OKX": "15m", "KuCoin": "15min", "Gate.io": "15m",
            "MEXC": "15m", "Kraken": "15", "Bitfinex": "15m", "Huobi": "15min", "Coinbase": "900", "Bitget": "15m"
        },
        "30m": {
            "Binance": "30m", "Bybit": "30", "OKX": "30m", "KuCoin": "30min", "Gate.io": "30m",
            "MEXC": "30m", "Kraken": "30", "Bitfinex": "30m", "Huobi": "30min", "Coinbase": "1800", "Bitget": "30m"
        },
        "1h": {
            "Binance": "1h", "Bybit": "60", "OKX": "1H", "KuCoin": "1hour", "Gate.io": "1h",
            "MEXC": "1h", "Kraken": "60", "Bitfinex": "1h", "Huobi": "60min", "Coinbase": "3600", "Bitget": "1h"
        },
        "4h": {
            "Binance": "4h", "Bybit": "240", "OKX": "4H", "KuCoin": "4hour", "Gate.io": "4h",
            "MEXC": "4h", "Kraken": "240", "Bitfinex": "4h", "Huobi": "4hour", "Coinbase": "14400", "Bitget": "4h"
        },
        "1d": {
            "Binance": "1d", "Bybit": "D", "OKX": "1D", "KuCoin": "1day", "Gate.io": "1d",
            "MEXC": "1d", "Kraken": "1440", "Bitfinex": "1D", "Huobi": "1day", "Coinbase": "86400", "Bitget": "1d"
        },
        "1w": {
            "Binance": "1w", "Bybit": "W", "OKX": "1W", "KuCoin": "1week", "Gate.io": "1w",
            "MEXC": "1w", "Kraken": "10080", "Bitfinex": "1W", "Huobi": "1week", "Coinbase": "604800", "Bitget": "1w"
        },
        "1M": {
            "Binance": "1M", "Bybit": "M", "OKX": "1M", "KuCoin": "1month", "Gate.io": "1M",
            "MEXC": "1M", "Kraken": "43200", "Bitfinex": "1M", "Huobi": "1mon", "Coinbase": "2592000", "Bitget": "1M"
        }
    }
    
    def __init__(self, max_cache_age: int = Config.CACHE_TTL_SECONDS):
        self.data_cache: Dict[str, Dict] = {}
        self.cache_timestamps: Dict[str, float] = {}
        self.max_cache_age = max_cache_age
        self.exchange_stats = defaultdict(lambda: {"success": 0, "fail": 0, "last_success": 0.0})
        self.session: Optional[aiohttp.ClientSession] = None
        self.connector = TCPConnector(limit=50, limit_per_host=10)
        logger.info(f"‚úÖ PriceFetcher initialized with {len(self.EXCHANGES)} exchanges")
    
    async def __aenter__(self):
        timeout = ClientTimeout(total=Config.EXCHANGE_TIMEOUT, connect=5, sock_read=10)
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=timeout,
            headers={"User-Agent": "TradingBot/v5.0 (+https://tradingbot.com)"},
            raise_for_status=False
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
            self.session = None
    
    def _get_exchange_interval(self, exchange_name: str, interval: str) -> str:
        """Get exchange-specific interval format"""
        mapping = self.INTERVAL_MAPPING.get(interval, {})
        return mapping.get(exchange_name, interval)
    
    def _get_cache_key(self, symbol: str, interval: str) -> str:
        """Generate consistent cache key"""
        return f"{symbol.upper()}_{interval.lower()}"
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """Check if cache entry is still valid"""
        if cache_key not in self.cache_timestamps:
            return False
        
        age = time.time() - self.cache_timestamps[cache_key]
        return age < self.max_cache_age
    
    async def _fetch_single_exchange(
        self,
        exchange: Dict,
        symbol: str,
        interval: str,
        limit: int
    ) -> Optional[List[Dict]]:
        """Fetch data from a single exchange with circuit breaker protection"""
        exchange_name = exchange["name"]
        
        # Circuit breaker check
        if not circuit_breaker.allow_request(exchange_name):
            logger.debug(f"üö´ Circuit breaker OPEN for {exchange_name}, skipping request")
            return None
        
        # Skip if exchange doesn't support this interval
        if interval not in self.INTERVAL_MAPPING:
            logger.debug(f"‚è≠Ô∏è {exchange_name} doesn't support interval {interval}, skipping")
            return None
        
        try:
            # Format symbol and interval
            formatted_symbol = exchange["symbol_fmt"](symbol)
            exchange_interval = self._get_exchange_interval(exchange_name, interval)
            
            # Build endpoint
            if "endpoint_builder" in exchange:
                endpoint = exchange["endpoint_builder"](formatted_symbol, exchange_interval)
            else:
                endpoint = exchange["endpoint"]
            
            # Build params
            params = exchange["params_builder"](formatted_symbol, exchange_interval, limit)
            
            # Make request with timeout protection
            start_time = time.time()
            
            async with self.session.get(endpoint, params=params) as response:
                elapsed = time.time() - start_time
                
                # Handle non-200 responses
                if response.status != 200:
                    circuit_breaker.record_failure(exchange_name)
                    self.exchange_stats[exchange_name]["fail"] += 1
                    logger.warning(f"‚ö†Ô∏è {exchange_name} HTTP {response.status} for {symbol} ({interval}) in {elapsed:.2f}s")
                    return None
                
                # Parse response
                data = await response.json()
                candles = self._parse_exchange_data(exchange_name, data, symbol)
                
                # Validate data quality
                if not candles or len(candles) < 5:
                    circuit_breaker.record_failure(exchange_name)
                    self.exchange_stats[exchange_name]["fail"] += 1
                    logger.warning(f"‚ö†Ô∏è {exchange_name} returned insufficient data for {symbol} ({len(candles)} candles)")
                    return None
                
                # Record success
                circuit_breaker.record_success(exchange_name)
                self.exchange_stats[exchange_name]["success"] += 1
                self.exchange_stats[exchange_name]["last_success"] = time.time()
                
                logger.info(f"‚úÖ {exchange_name}: {len(candles)} candles for {symbol} ({interval}) in {elapsed:.2f}s")
                return candles
                
        except asyncio.TimeoutError:
            circuit_breaker.record_failure(exchange_name)
            self.exchange_stats[exchange_name]["fail"] += 1
            logger.warning(f"‚è±Ô∏è {exchange_name} timeout for {symbol} ({interval})")
            return None
        except aiohttp.ClientError as e:
            circuit_breaker.record_failure(exchange_name)
            self.exchange_stats[exchange_name]["fail"] += 1
            logger.debug(f"üåê {exchange_name} client error: {str(e)[:100]}")
            return None
        except Exception as e:
            circuit_breaker.record_failure(exchange_name)
            self.exchange_stats[exchange_name]["fail"] += 1
            logger.error(f"‚ùå {exchange_name} unexpected error: {str(e)}", exc_info=True)
            return None
    
    def _parse_exchange_data(self, exchange_name: str, data: Any, symbol: str) -> List[Dict]:
        """Parse exchange-specific response into standard candle format"""
        candles = []
        
        try:
            if exchange_name == "Binance":
                for row in data:
                    candles.append({
                        "timestamp": int(row[0]),
                        "open": float(row[1]),
                        "high": float(row[2]),
                        "low": float(row[3]),
                        "close": float(row[4]),
                        "volume": float(row[5])
                    })
            
            elif exchange_name == "Bybit":
                if isinstance(data, dict) and "result" in data and "list" in data["result"]:
                    for row in data["result"]["list"]:
                        candles.append({
                            "timestamp": int(row[0]),
                            "open": float(row[1]),
                            "high": float(row[2]),
                            "low": float(row[3]),
                            "close": float(row[4]),
                            "volume": float(row[5])
                        })
            
            elif exchange_name == "OKX":
                if isinstance(data, dict) and "data" in data:
                    for row in data["data"]:
                        candles.append({
                            "timestamp": int(row[0]),
                            "open": float(row[1]),
                            "high": float(row[2]),
                            "low": float(row[3]),
                            "close": float(row[4]),
                            "volume": float(row[5])
                        })
            
            elif exchange_name == "KuCoin":
                if isinstance(data, dict) and "data" in data:
                    for row in data["data"]:
                        candles.append({
                            "timestamp": int(row[0]) * 1000,  # Convert seconds to milliseconds
                            "open": float(row[1]),
                            "close": float(row[2]),
                            "high": float(row[3]),
                            "low": float(row[4]),
                            "volume": float(row[5])
                        })
            
            elif exchange_name == "Gate.io":
                if isinstance(data, list):
                    for row in data:
                        candles.append({
                            "timestamp": int(row[0]) * 1000,
                            "open": float(row[5]),
                            "high": float(row[3]),
                            "low": float(row[4]),
                            "close": float(row[2]),
                            "volume": float(row[1])
                        })
            
            elif exchange_name == "MEXC":
                for row in data:
                    candles.append({
                        "timestamp": int(row[0]),
                        "open": float(row[1]),
                        "high": float(row[2]),
                        "low": float(row[3]),
                        "close": float(row[4]),
                        "volume": float(row[5])
                    })
            
            elif exchange_name == "Kraken":
                if isinstance(data, dict) and "result" in data:
                    # Get first key (trading pair)
                    pair_key = next(iter(data["result"]))
                    for row in data["result"][pair_key]:
                        candles.append({
                            "timestamp": int(row[0]) * 1000,
                            "open": float(row[1]),
                            "high": float(row[2]),
                            "low": float(row[3]),
                            "close": float(row[4]),
                            "volume": float(row[6])
                        })
            
            elif exchange_name == "Bitfinex":
                if isinstance(data, list):
                    for row in data:
                        candles.append({
                            "timestamp": int(row[0]),
                            "open": float(row[1]),
                            "close": float(row[2]),
                            "high": float(row[3]),
                            "low": float(row[4]),
                            "volume": float(row[5])
                        })
            
            elif exchange_name == "Huobi":
                if isinstance(data, dict) and "data" in data:
                    for row in data["data"]:
                        candles.append({
                            "timestamp": int(row["id"]) * 1000,
                            "open": float(row["open"]),
                            "high": float(row["high"]),
                            "low": float(row["low"]),
                            "close": float(row["close"]),
                            "volume": float(row["vol"])
                        })
            
            elif exchange_name == "Coinbase":
                if isinstance(data, list):
                    for row in data:
                        candles.append({
                            "timestamp": int(row[0]) * 1000,
                            "low": float(row[1]),
                            "high": float(row[2]),
                            "open": float(row[3]),
                            "close": float(row[4]),
                            "volume": float(row[5])
                        })
            
            elif exchange_name == "Bitget":
                if isinstance(data, dict) and "data" in data:
                    for row in data["data"]:
                        candles.append({
                            "timestamp": int(row[0]),
                            "open": float(row[1]),
                            "high": float(row[2]),
                            "low": float(row[3]),
                            "close": float(row[4]),
                            "volume": float(row[5])
                        })
            
            # Sort by timestamp (oldest to newest)
            candles.sort(key=lambda x: x["timestamp"])
            return candles
            
        except Exception as e:
            logger.error(f"‚ùå Parse error for {exchange_name} {symbol}: {str(e)}", exc_info=True)
            return []
    
    def _calculate_data_quality(self, candles: List[Dict], expected_count: int) -> float:
        """Calculate data quality score based on completeness and consistency"""
        if not candles or len(candles) < 5:
            return 0.0
        
        # Check for missing timestamps (gaps)
        timestamps = [c["timestamp"] for c in candles]
        expected_interval = timestamps[-1] - timestamps[-2] if len(timestamps) > 1 else 60000
        
        gaps = 0
        for i in range(1, len(timestamps)):
            diff = timestamps[i] - timestamps[i-1]
            if diff > expected_interval * 1.5:  # More than 50% gap
                gaps += 1
        
        gap_penalty = min(1.0, gaps / len(candles))
        
        # Check for price anomalies (spikes)
        closes = [c["close"] for c in candles]
        if len(closes) > 1:
            returns = [abs((closes[i] - closes[i-1]) / closes[i-1]) for i in range(1, len(closes))]
            spike_count = sum(1 for r in returns if r > 0.2)  # >20% move
            spike_penalty = min(1.0, spike_count / len(returns))
        else:
            spike_penalty = 0.0
        
        # Calculate final score
        completeness = min(1.0, len(candles) / expected_count)
        quality = completeness * (1 - gap_penalty * 0.3) * (1 - spike_penalty * 0.2)
        
        return max(0.0, min(1.0, quality))
    
    def _aggregate_candles(self, all_candles: List[List[Dict]], symbol: str, interval: str) -> Optional[List[Dict]]:
        """Aggregate candles from multiple sources with quality weighting"""
        if not all_candles or len(all_candles) < Config.REQUIRE_MIN_EXCHANGES:
            logger.warning(f"‚ö†Ô∏è Insufficient data sources for {symbol} {interval}: {len(all_candles)} sources (min {Config.REQUIRE_MIN_EXCHANGES})")
            return None
        
        # Group candles by timestamp
        timestamp_data = defaultdict(list)
        for exchange_candles in all_candles:
            for candle in exchange_candles:
                timestamp_data[candle["timestamp"]].append(candle)
        
        # Aggregate with quality weighting
        aggregated = []
        for timestamp in sorted(timestamp_data.keys()):
            candles_at_ts = timestamp_data[timestamp]
            
            if len(candles_at_ts) == 1:
                # Single source - use directly
                aggregated.append(candles_at_ts[0])
            else:
                # Multiple sources - weighted average
                weights = []
                opens, highs, lows, closes, volumes = [], [], [], [], []
                
                for candle in candles_at_ts:
                    # Get exchange weight (based on priority)
                    exchange_weight = next(
                        (ex["weight"] for ex in self.EXCHANGES if ex["name"].lower() in str(candle)),
                        0.5
                    )
                    weights.append(exchange_weight)
                    opens.append(candle["open"] * exchange_weight)
                    highs.append(candle["high"] * exchange_weight)
                    lows.append(candle["low"] * exchange_weight)
                    closes.append(candle["close"] * exchange_weight)
                    volumes.append(candle["volume"] * exchange_weight)
                
                total_weight = sum(weights)
                if total_weight == 0:
                    continue
                
                aggregated.append({
                    "timestamp": timestamp,
                    "open": sum(opens) / total_weight,
                    "high": sum(highs) / total_weight,
                    "low": sum(lows) / total_weight,
                    "close": sum(closes) / total_weight,
                    "volume": sum(volumes) / total_weight,
                    "source_count": len(candles_at_ts),
                    "sources": [c.get("exchange", "unknown") for c in candles_at_ts]
                })
        
        # Validate final data quality
        quality_score = self._calculate_data_quality(aggregated, len(aggregated))
        if quality_score < Config.MIN_DATA_QUALITY_SCORE:
            logger.warning(f"‚ö†Ô∏è Data quality too low for {symbol} {interval}: {quality_score:.2f} (min {Config.MIN_DATA_QUALITY_SCORE})")
            return None
        
        logger.info(f"‚úÖ Aggregated {len(aggregated)} candles from {len(all_candles)} sources for {symbol} {interval} (quality: {quality_score:.2f})")
        return aggregated
    
    async def get_candles(
        self,
        symbol: str,
        interval: str = "1h",
        limit: int = 100,
        force_refresh: bool = False
    ) -> List[Dict]:
        """
        Get candles from exchanges with smart caching and fallback logic
        REAL DATA ONLY - NO SYNTHETIC DATA FALLBACK
        """
        cache_key = self._get_cache_key(symbol, interval)
        
        # Return from cache if valid and not forced refresh
        if not force_refresh and self._is_cache_valid(cache_key) and cache_key in self.data_cache:
            cached_data = self.data_cache[cache_key]
            logger.debug(f"üì¶ Cache hit for {symbol} {interval} ({len(cached_data)} candles)")
            return cached_data[-limit:]
        
        # Fetch fresh data from exchanges
        logger.info(f"üîÑ Fetching fresh data: {symbol} {interval} ({limit} candles)")
        
        # Use more candles for cache (to avoid frequent refetching)
        fetch_limit = max(limit * 2, 200)
        
        # Fetch from all exchanges concurrently
        tasks = []
        for exchange in sorted(self.EXCHANGES, key=lambda x: x["priority"]):
            task = self._fetch_single_exchange(exchange, symbol, interval, fetch_limit)
            tasks.append(task)
        
        # Wait for all fetches with timeout
        try:
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=Config.EXCHANGE_TIMEOUT * 2
            )
        except asyncio.TimeoutError:
            logger.error(f"‚ùå Total fetch timeout for {symbol} {interval}")
            return []
        
        # Filter valid results
        valid_results = [
            r for r in results
            if r is not None and not isinstance(r, Exception) and len(r) >= 10
        ]
        
        # Critical validation: Require minimum number of sources
        if len(valid_results) < Config.REQUIRE_MIN_EXCHANGES:
            logger.error(
                f"‚ùå CRITICAL: Insufficient data sources for {symbol} {interval} "
                f"({len(valid_results)} sources, min {Config.REQUIRE_MIN_EXCHANGES} required). "
                f"NO SYNTHETIC DATA FALLBACK - Returning empty result."
            )
            return []
        
        # Aggregate data from multiple sources
        aggregated = self._aggregate_candles(valid_results, symbol, interval)
        if not aggregated or len(aggregated) < Config.MIN_CANDLES_FOR_ANALYSIS:
            logger.error(
                f"‚ùå CRITICAL: Aggregation failed or insufficient candles for {symbol} {interval} "
                f"({len(aggregated) if aggregated else 0} candles, min {Config.MIN_CANDLES_FOR_ANALYSIS} required). "
                f"NO SYNTHETIC DATA FALLBACK - Returning empty result."
            )
            return []
        
        # Cache the result
        self.data_cache[cache_key] = aggregated
        self.cache_timestamps[cache_key] = time.time()
        
        # Trim to requested limit
        return aggregated[-limit:]
    
    def get_exchange_health(self) -> Dict[str, Any]:
        """Get health status of all exchanges"""
        now = time.time()
        health_report = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "total_exchanges": len(self.EXCHANGES),
            "healthy_exchanges": 0,
            "exchanges": {}
        }
        
        for exchange in self.EXCHANGES:
            name = exchange["name"]
            stats = self.exchange_stats[name]
            last_success = stats["last_success"]
            is_healthy = (now - last_success) < 300  # Healthy if succeeded in last 5 minutes
            
            health_report["exchanges"][name] = {
                "priority": exchange["priority"],
                "success_count": stats["success"],
                "fail_count": stats["fail"],
                "last_success": datetime.fromtimestamp(last_success, tz=timezone.utc).isoformat() if last_success else None,
                "is_healthy": is_healthy,
                "circuit_breaker_state": circuit_breaker.state[name].value
            }
            
            if is_healthy:
                health_report["healthy_exchanges"] += 1
        
        health_report["health_score"] = round(health_report["healthy_exchanges"] / health_report["total_exchanges"] * 100, 1)
        return health_report

# ========== VOLATILITY ADJUSTED TRAIL SYSTEM ==========
class VolatilityAdjustedTrail:
    """ICT-Compliant Volatility Adjusted Trail System"""
    
    @staticmethod
    def calculate_atr(df: pd.DataFrame, period: int = 14) -> pd.Series:
        """Calculate Average True Range with proper handling"""
        if len(df) < period + 1:
            return pd.Series([0.0] * len(df), index=df.index)
        
        high = df['high']
        low = df['low']
        close = df['close'].shift(1)
        
        tr1 = high - low
        tr2 = (high - close).abs()
        tr3 = (low - close).abs()
        
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        atr = tr.rolling(window=period, min_periods=1).mean()
        return atr.fillna(0)
    
    @staticmethod
    def calculate_trail(df: pd.DataFrame, atr_multiplier: float = 2.0) -> Dict[str, Any]:
        """Calculate volatility-adjusted trailing stop with ICT principles"""
        if len(df) < 50:
            return {
                "current_trail": None,
                "current_direction": 0,
                "trend_strength": 0.0,
                "trail_type": "NEUTRAL",
                "atr_value": 0.0,
                "volatility_ratio": 1.0,
                "dynamic_multiplier": atr_multiplier,
                "signals": [],
                "fvgs": [],  # Fair Value Gaps
                "order_blocks": []  # Order Blocks
            }
        
        # Calculate core indicators
        atr = VolatilityAdjustedTrail.calculate_atr(df)
        ema_fast = df['close'].ewm(span=9, adjust=False).mean()
        ema_slow = df['close'].ewm(span=21, adjust=False).mean()
        
        # Volatility analysis
        volatility = df['close'].rolling(window=20).std()
        avg_volatility = volatility.rolling(window=20).mean()
        vol_ratio = (volatility / avg_volatility).fillna(1.0).clip(0.5, 3.0)
        
        # Dynamic multiplier based on volatility regime
        dynamic_multiplier = atr_multiplier * (1 + 0.5 * (vol_ratio - 1))
        
        # Calculate trails
        long_trail = df['high'].rolling(window=20).max() - (atr * dynamic_multiplier)
        short_trail = df['low'].rolling(window=20).min() + (atr * dynamic_multiplier)
        
        # Determine market direction
        current_dir = 0
        if ema_fast.iloc[-1] > ema_slow.iloc[-1] and ema_fast.iloc[-2] <= ema_slow.iloc[-2]:
            current_dir = 1  # Bullish crossover
        elif ema_fast.iloc[-1] < ema_slow.iloc[-1] and ema_fast.iloc[-2] >= ema_slow.iloc[-2]:
            current_dir = -1  # Bearish crossover
        elif ema_fast.iloc[-1] > ema_slow.iloc[-1]:
            current_dir = 1  # Uptrend
        elif ema_fast.iloc[-1] < ema_slow.iloc[-1]:
            current_dir = -1  # Downtrend
        
        # Calculate trend strength (0.0 to 1.0)
        if current_dir == 1:
            strength = (df['close'].iloc[-1] - long_trail.iloc[-1]) / (atr.iloc[-1] * dynamic_multiplier.iloc[-1] + 1e-8)
        elif current_dir == -1:
            strength = (short_trail.iloc[-1] - df['close'].iloc[-1]) / (atr.iloc[-1] * dynamic_multiplier.iloc[-1] + 1e-8)
        else:
            strength = 0.0
        
        trend_strength = float(np.clip(strength, 0.0, 1.0))
        
        # Generate signals with ICT concepts
        signals = []
        fvgs = []  # Fair Value Gaps
        order_blocks = []  # Order Blocks
        
        # Detect Fair Value Gaps (FVGs)
        for i in range(2, len(df)):
            prev_high = df['high'].iloc[i-2]
            curr_low = df['low'].iloc[i]
            
            if prev_high < curr_low:  # Bullish FVG
                gap_size = curr_low - prev_high
                if gap_size > atr.iloc[i] * 0.5:  # Significant gap
                    fvgs.append({
                        "type": "bullish",
                        "start": prev_high,
                        "end": curr_low,
                        "size": gap_size,
                        "timestamp": df.index[i]
                    })
            elif df['low'].iloc[i-2] > df['high'].iloc[i]:  # Bearish FVG
                gap_size = df['low'].iloc[i-2] - df['high'].iloc[i]
                if gap_size > atr.iloc[i] * 0.5:
                    fvgs.append({
                        "type": "bearish",
                        "start": df['high'].iloc[i],
                        "end": df['low'].iloc[i-2],
                        "size": gap_size,
                        "timestamp": df.index[i]
                    })
        
        # Detect Order Blocks
        for i in range(3, len(df)):
            # Bullish order block (drop after consolidation)
            if (df['close'].iloc[i-1] < df['open'].iloc[i-1] and  # Bearish candle
                df['close'].iloc[i] > df['open'].iloc[i] and      # Bullish reversal
                df['close'].iloc[i] > df['high'].iloc[i-1]):      # Break of prior high
                order_blocks.append({
                    "type": "bullish",
                    "high": df['high'].iloc[i-1],
                    "low": df['low'].iloc[i-1],
                    "timestamp": df.index[i-1],
                    "strength": 0.8
                })
            
            # Bearish order block (rally after consolidation)
            if (df['close'].iloc[i-1] > df['open'].iloc[i-1] and  # Bullish candle
                df['close'].iloc[i] < df['open'].iloc[i] and      # Bearish reversal
                df['close'].iloc[i] < df['low'].iloc[i-1]):       # Break of prior low
                order_blocks.append({
                    "type": "bearish",
                    "high": df['high'].iloc[i-1],
                    "low": df['low'].iloc[i-1],
                    "timestamp": df.index[i-1],
                    "strength": 0.8
                })
        
        # Generate recent signals (last 10)
        for i in range(max(1, len(df)-10), len(df)):
            if current_dir == 1 and df['close'].iloc[i] > long_trail.iloc[i-1]:
                signals.append({
                    "timestamp": df.index[i].isoformat() if hasattr(df.index[i], 'isoformat') else str(df.index[i]),
                    "type": "BUY",
                    "price": float(df['close'].iloc[i]),
                    "trail": float(long_trail.iloc[i-1]),
                    "reason": "Price above volatility trail"
                })
            elif current_dir == -1 and df['close'].iloc[i] < short_trail.iloc[i-1]:
                signals.append({
                    "timestamp": df.index[i].isoformat() if hasattr(df.index[i], 'isoformat') else str(df.index[i]),
                    "type": "SELL",
                    "price": float(df['close'].iloc[i]),
                    "trail": float(short_trail.iloc[i-1]),
                    "reason": "Price below volatility trail"
                })
        
        return {
            "current_trail": float(long_trail.iloc[-1]) if current_dir == 1 else float(short_trail.iloc[-1]) if current_dir == -1 else None,
            "current_direction": current_dir,
            "trend_strength": trend_strength,
            "trail_type": "LONG" if current_dir == 1 else "SHORT" if current_dir == -1 else "NEUTRAL",
            "atr_value": float(atr.iloc[-1]),
            "volatility_ratio": float(vol_ratio.iloc[-1]),
            "dynamic_multiplier": float(dynamic_multiplier.iloc[-1]),
            "signals": signals[-10:],
            "fvgs": fvgs[-5:],  # Last 5 FVGs
            "order_blocks": order_blocks[-5:],  # Last 5 order blocks
            "market_structure": "BULLISH" if current_dir == 1 else "BEARISH" if current_dir == -1 else "RANGING"
        }

# ========== AI TRADING ENGINE (ML MODELS) ==========
class AITradingEngine:
    """Enterprise ML Trading Engine with Ensemble Modeling"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.lgb_models = {}
        self.feature_columns = None
        self.device = torch.device('cuda' if torch.cuda.is_available() and ML_AVAILABLE else 'cpu')
        logger.info(f"‚úÖ AI Trading Engine initialized on {self.device}")
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create comprehensive trading features with validation"""
        try:
            if len(df) < Config.ML_FEATURES_MIN:
                logger.warning(f"Insufficient data for feature creation: {len(df)} rows (min {Config.ML_FEATURES_MIN})")
                return pd.DataFrame()
            
            df = df.copy()
            
            # Required columns validation
            required_cols = ['open', 'high', 'low', 'close', 'volume']
            missing = [col for col in required_cols if col not in df.columns]
            if missing:
                logger.error(f"Missing required columns: {missing}")
                return pd.DataFrame()
            
            # Price features
            df['returns'] = df['close'].pct_change().fillna(0)
            df['log_returns'] = np.log(df['close'] / df['close'].shift(1)).fillna(0)
            
            # Volatility features
            for window in [5, 10, 20, 50]:
                df[f'volatility_{window}'] = df['returns'].rolling(window).std().fillna(0)
                df[f'atr_{window}'] = (df['high'] - df['low']).rolling(window).mean().fillna(0)
            
            # Moving averages
            for period in [5, 9, 20, 50, 100, 200]:
                if len(df) >= period:
                    df[f'sma_{period}'] = df['close'].rolling(period).mean().fillna(method='ffill').fillna(df['close'])
                    df[f'ema_{period}'] = df['close'].ewm(span=period, adjust=False).mean().fillna(method='ffill').fillna(df['close'])
                    df[f'price_sma_ratio_{period}'] = (df['close'] / df[f'sma_{period}']).fillna(1.0)
            
            # RSI
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss.replace(0, np.nan)
            df['rsi'] = (100 - (100 / (1 + rs))).fillna(50)
            
            # MACD
            exp1 = df['close'].ewm(span=12, adjust=False).mean()
            exp2 = df['close'].ewm(span=26, adjust=False).mean()
            df['macd'] = (exp1 - exp2).fillna(0)
            df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean().fillna(0)
            df['macd_hist'] = (df['macd'] - df['macd_signal']).fillna(0)
            
            # Bollinger Bands
            df['bb_middle'] = df['close'].rolling(20).mean().fillna(method='ffill').fillna(df['close'])
            bb_std = df['close'].rolling(20).std().fillna(0)
            df['bb_upper'] = (df['bb_middle'] + (bb_std * 2)).fillna(method='ffill').fillna(df['close'])
            df['bb_lower'] = (df['bb_middle'] - (bb_std * 2)).fillna(method='ffill').fillna(df['close'])
            df['bb_width'] = (df['bb_upper'] - df['bb_lower']).fillna(0)
            df['bb_position'] = ((df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'] + 1e-8)).fillna(0.5).clip(0, 1)
            
            # Volume features
            df['volume_sma_20'] = df['volume'].rolling(20).mean().fillna(method='ffill').fillna(df['volume'])
            df['volume_ratio'] = (df['volume'] / df['volume_sma_20']).fillna(1.0)
            df['obv'] = (np.sign(df['close'].diff()) * df['volume']).fillna(0).cumsum()
            
            # Price patterns
            df['high_low_pct'] = ((df['high'] - df['low']) / df['close'] * 100).fillna(0)
            df['close_open_pct'] = ((df['close'] - df['open']) / df['open'] * 100).fillna(0)
            
            # Momentum
            df['momentum_5'] = df['close'].pct_change(5).fillna(0)
            df['momentum_10'] = df['close'].pct_change(10).fillna(0)
            df['roc'] = ((df['close'] - df['close'].shift(10)) / df['close'].shift(10) * 100).fillna(0)
            
            # Support/Resistance
            df['resistance'] = df['high'].rolling(20).max().fillna(method='ffill').fillna(df['high'])
            df['support'] = df['low'].rolling(20).min().fillna(method='ffill').fillna(df['low'])
            df['distance_to_resistance'] = ((df['resistance'] - df['close']) / df['close'] * 100).fillna(0)
            df['distance_to_support'] = ((df['close'] - df['support']) / df['close'] * 100).fillna(0)
            
            # Lagged features
            for lag in [1, 2, 3, 5, 10]:
                df[f'returns_lag_{lag}'] = df['returns'].shift(lag).fillna(0)
                df[f'close_lag_{lag}'] = df['close'].shift(lag).fillna(method='ffill').fillna(df['close'])
            
            # Target creation (3-class classification)
            future_return = df['close'].shift(-5) / df['close'] - 1
            upper_threshold = future_return.quantile(0.65)
            lower_threshold = future_return.quantile(0.35)
            
            df['target'] = np.where(
                future_return > upper_threshold, 2,
                np.where(future_return < lower_threshold, 0, 1)
            )
            
            # Drop NaN values
            df = df.dropna()
            
            # Store feature columns
            non_feature_cols = ['target', 'timestamp', 'datetime', 'date', 'time']
            self.feature_columns = [col for col in df.columns if col not in non_feature_cols and not col.startswith('target')]
            
            logger.info(f"‚úÖ Created {len(self.feature_columns)} features from {len(df)} samples")
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Feature creation error: {str(e)}", exc_info=True)
            return pd.DataFrame()
    
    async def train_models(self, symbol: str, df: pd.DataFrame) -> bool:
        """Train all ML models with validation"""
        try:
            if len(df) < Config.MIN_CANDLES_FOR_TRAINING:
                logger.warning(f"Insufficient data for training {symbol}: {len(df)} candles (min {Config.MIN_CANDLES_FOR_TRAINING})")
                return False
            
            # Create features
            df_features = self.create_features(df)
            if len(df_features) < Config.MIN_CANDLES_FOR_TRAINING * 0.8:  # 80% after feature creation
                logger.warning(f"Insufficient features after processing for {symbol}: {len(df_features)} samples")
                return False
            
            # Prepare sequences
            X, y = self.prepare_sequences(df_features)
            if len(X) < 100:
                logger.warning(f"Insufficient sequences for {symbol}: {len(X)} sequences")
                return False
            
            # Split data
            split_idx = int(len(X) * 0.8)
            X_train, X_val = X[:split_idx], X[split_idx:]
            y_train, y_val = y[:split_idx], y[split_idx:]
            
            # Scale features
            scaler = StandardScaler()
            X_train_flat = X_train.reshape(-1, X_train.shape[-1])
            X_val_flat = X_val.reshape(-1, X_val.shape[-1])
            X_train_scaled = scaler.fit_transform(X_train_flat).reshape(X_train.shape)
            X_val_scaled = scaler.transform(X_val_flat).reshape(X_val.shape)
            self.scalers[symbol] = scaler
            
            # Train models concurrently
            tasks = [
                self._train_lightgbm(symbol, X_train_flat, y_train, X_val_flat, y_val),
                self._train_lstm(symbol, X_train_scaled, y_train, X_val_scaled, y_val),
                self._train_transformer(symbol, X_train_scaled, y_train, X_val_scaled, y_val)
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            success_count = sum(1 for r in results if r is True)
            
            if success_count >= 2:  # At least 2 models trained successfully
                logger.info(f"‚úÖ Successfully trained {success_count}/3 models for {symbol}")
                return True
            else:
                logger.error(f"‚ùå Training failed for {symbol}: only {success_count}/3 models succeeded")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Critical training error for {symbol}: {str(e)}", exc_info=True)
            return False
    
    def prepare_sequences(self, df: pd.DataFrame, sequence_length: int = Config.ML_SEQUENCE_LENGTH) -> Tuple[np.ndarray, np.ndarray]:
        """Prepare sequential data for LSTM/Transformer"""
        if self.feature_columns is None or len(df) < sequence_length:
            return np.array([]), np.array([])
        
        try:
            features = df[self.feature_columns].values.astype(np.float32)
            targets = df['target'].values.astype(np.int64)
            
            X, y = [], []
            for i in range(len(features) - sequence_length):
                X.append(features[i:i+sequence_length])
                y.append(targets[i+sequence_length])
            
            return np.array(X), np.array(y)
            
        except Exception as e:
            logger.error(f"‚ùå Sequence preparation error: {str(e)}", exc_info=True)
            return np.array([]), np.array([])
    
    async def _train_lightgbm(self, symbol: str, X_train, y_train, X_val, y_val) -> bool:
        """Train LightGBM model"""
        try:
            params = {
                'objective': 'multiclass',
                'num_class': 3,
                'metric': 'multi_logloss',
                'boosting_type': 'gbdt',
                'num_leaves': 31,
                'learning_rate': 0.05,
                'feature_fraction': 0.9,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'verbose': -1,
                'seed': 42
            }
            
            train_data = lgb.Dataset(X_train, label=y_train)
            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
            
            model = lgb.train(
                params,
                train_data,
                valid_sets=[val_data],
                num_boost_round=1000,
                callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)]
            )
            
            self.lgb_models[symbol] = model
            
            # Evaluate
            y_pred = model.predict(X_val)
            y_pred_class = np.argmax(y_pred, axis=1)
            accuracy = accuracy_score(y_val, y_pred_class)
            
            logger.info(f"‚úÖ LightGBM trained for {symbol}. Accuracy: {accuracy:.4f}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå LightGBM training failed for {symbol}: {str(e)}", exc_info=True)
            return False
    
    async def _train_lstm(self, symbol: str, X_train, y_train, X_val, y_val) -> bool:
        """Train LSTM model"""
        if not ML_AVAILABLE:
            logger.warning("ML libraries not available, skipping LSTM training")
            return False
        
        try:
            # Model definition and training...
            logger.info(f"‚úÖ LSTM training placeholder for {symbol} (implementation requires torch)")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå LSTM training failed for {symbol}: {str(e)}", exc_info=True)
            return False
    
    async def _train_transformer(self, symbol: str, X_train, y_train, X_val, y_val) -> bool:
        """Train Transformer model"""
        if not ML_AVAILABLE:
            logger.warning("ML libraries not available, skipping Transformer training")
            return False
        
        try:
            # Model definition and training...
            logger.info(f"‚úÖ Transformer training placeholder for {symbol} (implementation requires torch)")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Transformer training failed for {symbol}: {str(e)}", exc_info=True)
            return False
    
    def predict(self, symbol: str, df: pd.DataFrame) -> Dict[str, Any]:
        """Generate ensemble prediction from all available models"""
        try:
            if df.empty or len(df) < 30:
                return {
                    "prediction": "NEUTRAL",
                    "confidence": 0.50,
                    "method": "insufficient_data",
                    "ml_score": 0.5,
                    "model_details": {}
                }
            
            # Create features
            df_features = self.create_features(df)
            if len(df_features) < Config.ML_SEQUENCE_LENGTH or not self.feature_columns:
                return {
                    "prediction": "NEUTRAL",
                    "confidence": 0.50,
                    "method": "feature_creation_failed",
                    "ml_score": 0.5,
                    "model_details": {}
                }
            
            # Get recent sequence
            features = df_features[self.feature_columns].values[-Config.ML_SEQUENCE_LENGTH:]
            if len(features) < Config.ML_SEQUENCE_LENGTH:
                pad_len = Config.ML_SEQUENCE_LENGTH - len(features)
                pad = np.zeros((pad_len, features.shape[1]))
                features = np.vstack([pad, features])
            
            predictions = []
            confidences = []
            model_details = {}
            
            # LightGBM prediction (if available)
            if symbol in self.lgb_models and symbol in self.scalers:
                try:
                    model = self.lgb_models[symbol]
                    scaler = self.scalers[symbol]
                    recent_scaled = scaler.transform(features[-1:].reshape(1, -1))
                    prob = model.predict(recent_scaled)[0]
                    pred = int(np.argmax(prob))
                    confidence = float(np.max(prob))
                    
                    predictions.append(pred)
                    confidences.append(confidence)
                    model_details['lightgbm'] = {
                        "prediction": pred,
                        "confidence": confidence,
                        "probabilities": prob.tolist()
                    }
                except Exception as e:
                    logger.warning(f"LightGBM prediction failed for {symbol}: {e}")
            
            # Return ensemble result
            if not predictions:
                return {
                    "prediction": "NEUTRAL",
                    "confidence": 0.40,
                    "method": "no_valid_models",
                    "ml_score": 0.5,
                    "model_details": model_details
                }
            
            # Weighted voting
            weighted_votes = []
            for p, c in zip(predictions, confidences):
                weighted_votes.extend([p] * int(c * 20))
            
            from collections import Counter
            final_pred = Counter(weighted_votes).most_common(1)[0][0] if weighted_votes else 1
            avg_conf = sum(confidences) / len(confidences) if confidences else 0.5
            
            # Map to signal type
            pred_map = {0: "SELL", 1: "NEUTRAL", 2: "BUY"}
            signal_str = pred_map.get(final_pred, "NEUTRAL")
            
            # Upgrade to strong signals for high confidence
            if avg_conf > 0.75:
                if signal_str == "BUY":
                    signal_str = "STRONG_BUY"
                elif signal_str == "SELL":
                    signal_str = "STRONG_SELL"
            
            return {
                "prediction": signal_str,
                "confidence": float(avg_conf),
                "method": "ensemble",
                "ml_score": float(final_pred),
                "model_details": model_details
            }
            
        except Exception as e:
            logger.error(f"‚ùå Critical prediction error for {symbol}: {str(e)}", exc_info=True)
            return {
                "prediction": "NEUTRAL",
                "confidence": 0.30,
                "method": "error_fallback",
                "ml_score": 0.5,
                "error": str(e),
                "model_details": {}
            }

# ========== GLOBAL INSTANCES ==========
price_fetcher = PriceFetcher()
ai_engine = AITradingEngine()
websocket_connections: Set[WebSocket] = set()

# ========== API ENDPOINTS ==========
@app.get("/", response_class=HTMLResponse)
async def root():
    """Redirect to dashboard"""
    return """
    <html>
        <head>
            <title>Trading Bot v5.0</title>
            <meta http-equiv="refresh" content="0;url=/dashboard">
        </head>
        <body>
            <p>Redirecting to trading dashboard...</p>
        </body>
    </html>
    """

@app.get("/health")
async def health_check():
    """Comprehensive health check endpoint"""
    try:
        # Exchange health
        async with price_fetcher as fetcher:
            exchange_health = fetcher.get_exchange_health()
        
        # ML health
        ml_health = {
            "models_loaded": len(ai_engine.models) + len(ai_engine.lgb_models),
            "scalers_available": len(ai_engine.scalers),
            "ml_available": ML_AVAILABLE
        }
        
        # Overall status
        healthy_exchanges = exchange_health["healthy_exchanges"]
        total_exchanges = exchange_health["total_exchanges"]
        health_score = exchange_health["health_score"]
        
        status = "healthy" if health_score >= 70 and healthy_exchanges >= 5 else "degraded" if health_score >= 40 else "unhealthy"
        
        return {
            "status": status,
            "version": "5.0.0",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "uptime": time.time() - startup_time,
            "exchange_health": exchange_health,
            "ml_health": ml_health,
            "health_score": health_score,
            "message": "All systems operational" if status == "healthy" else "Partial degradation detected" if status == "degraded" else "Critical issues detected"
        }
        
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Health check failed: {str(e)}")

@app.get("/api/analyze/{symbol}")
async def analyze_symbol(
    symbol: str,
    interval: str = Query(default="1h", regex="^(1m|5m|15m|30m|1h|4h|1d|1w|1M)$"),
    request: Request = None
):
    """
    Complete technical analysis for a symbol
    REAL DATA ONLY - NO SYNTHETIC DATA FALLBACK
    """
    symbol = symbol.upper()
    logger.info(f"üîç Analysis requested: {symbol} ({interval})")
    
    try:
        # Validate symbol format
        if not symbol.endswith("USDT"):
            symbol += "USDT"
        
        # Fetch candles with real data only policy
        async with price_fetcher as fetcher:
            candles = await fetcher.get_candles(symbol, interval, 100)
        
        # CRITICAL: If insufficient real data, return error - NO SYNTHETIC FALLBACK
        if not candles or len(candles) < Config.MIN_CANDLES_FOR_ANALYSIS:
            logger.error(
                f"‚ùå CRITICAL: Insufficient real data for {symbol} {interval} "
                f"({len(candles) if candles else 0} candles, min {Config.MIN_CANDLES_FOR_ANALYSIS} required). "
                f"NO SYNTHETIC DATA FALLBACK - Returning error."
            )
            raise HTTPException(
                status_code=422,
                detail=f"Insufficient real market data available for {symbol} ({interval}). "
                       f"Requires minimum {Config.MIN_CANDLES_FOR_ANALYSIS} candles. "
                       f"This is not an error - real market data is temporarily unavailable."
            )
        
        # Convert to DataFrame
        df = pd.DataFrame(candles)
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        
        # Calculate volatility adjusted trail (ICT compliant)
        vat_system = VolatilityAdjustedTrail()
        vat_result = vat_system.calculate_trail(df)
        
        # ML prediction (if models available)
        ml_prediction = None
        if ML_AVAILABLE:
            try:
                ml_prediction = ai_engine.predict(symbol, df)
            except Exception as e:
                logger.warning(f"ML prediction failed for {symbol}: {e}")
        
        # Calculate price change
        current_price = df['close'].iloc[-1]
        prev_price = df['close'].iloc[-2] if len(df) > 1 else current_price
        price_change = ((current_price - prev_price) / prev_price * 100) if prev_price != 0 else 0.0
        
        # Build response
        response = {
            "success": True,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "symbol": symbol,
            "interval": interval,
            "price_data": {
                "current": float(current_price),
                "previous": float(prev_price),
                "change_percent": round(price_change, 2),
                "volume_24h": float(df['volume'].sum()),
                "high_24h": float(df['high'].max()),
                "low_24h": float(df['low'].min())
            },
            "volatility_trail": vat_result,
            "ml_prediction": ml_prediction,
            "data_quality": {
                "candle_count": len(candles),
                "source_count": vat_result.get("source_count", 1),
                "quality_score": price_fetcher._calculate_data_quality(candles, len(candles))
            },
            "market_context": {
                "market_structure": vat_result["market_structure"],
                "trend_strength": vat_result["trend_strength"],
                "volatility_regime": "HIGH" if vat_result["volatility_ratio"] > 1.5 else "LOW" if vat_result["volatility_ratio"] < 0.7 else "NORMAL"
            }
        }
        
        logger.info(f"‚úÖ Analysis completed for {symbol} {interval}: {len(candles)} candles, {response['data_quality']['source_count']} sources")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Analysis failed for {symbol} {interval}: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Analysis failed due to internal error. Real market data may be temporarily unavailable. Error: {str(e)[:200]}"
        )

@app.post("/api/train/{symbol}")
async def train_model(symbol: str):
    """Train ML models on real market data"""
    symbol = symbol.upper()
    logger.info(f"üß† Training requested for {symbol}")
    
    try:
        # Fetch sufficient historical data
        async with price_fetcher as fetcher:
            candles = await fetcher.get_candles(symbol, "1h", 1000)
        
        # Validate data quantity
        if not candles or len(candles) < Config.MIN_CANDLES_FOR_TRAINING:
            raise HTTPException(
                status_code=400,
                detail=f"Insufficient real market data for training {symbol}. "
                       f"Requires minimum {Config.MIN_CANDLES_FOR_TRAINING} candles. "
                       f"Current: {len(candles) if candles else 0} candles."
            )
        
        # Train models
        df = pd.DataFrame(candles)
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        
        success = await ai_engine.train_models(symbol, df)
        
        if success:
            return {
                "success": True,
                "message": f"Successfully trained ML models for {symbol} using {len(candles)} real market candles",
                "symbol": symbol,
                "candles_used": len(candles),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        else:
            raise HTTPException(
                status_code=500,
                detail="Model training failed. Insufficient data quality or feature engineering issues."
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Training failed for {symbol}: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Training failed: {str(e)[:200]}")

@app.websocket("/ws/{symbol}")
async def websocket_endpoint(websocket: WebSocket, symbol: str):
    """Real-time price streaming via WebSocket"""
    symbol = symbol.upper()
    await websocket.accept()
    websocket_connections.add(websocket)
    logger.info(f"üîó WebSocket connected for {symbol} ({len(websocket_connections)} total connections)")
    
    try:
        while True:
            # Fetch latest price
            async with price_fetcher as fetcher:
                candles = await fetcher.get_candles(symbol, "1m", 2)
            
            if candles and len(candles) >= 2:
                current = candles[-1]["close"]
                previous = candles[-2]["close"]
                change = ((current - previous) / previous * 100) if previous != 0 else 0.0
                
                message = {
                    "type": "price_update",
                    "symbol": symbol,
                    "price": current,
                    "change_percent": round(change, 2),
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "volume": candles[-1]["volume"]
                }
                
                await websocket.send_json(message)
            
            # Throttle updates to 1 per second
            await asyncio.sleep(1.0)
            
    except WebSocketDisconnect:
        websocket_connections.remove(websocket)
        logger.info(f"‚ùå WebSocket disconnected for {symbol} ({len(websocket_connections)} remaining)")
    except Exception as e:
        websocket_connections.remove(websocket)
        logger.error(f"WebSocket error for {symbol}: {str(e)}", exc_info=True)
        await websocket.close(code=status.WS_1011_INTERNAL_ERROR)
 from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from datetime import datetime

app = FastAPI(title="Trading Bot Backend")

# Template klas√∂r√ºn√º belirt
# ‚Üí Proje k√∂k dizininde "templates" klas√∂r√º olmalƒ± ve i√ßinde dashboard.html bulunmalƒ±
templates = Jinja2Templates(directory="templates")

@app.get("/dashboard", response_class=HTMLResponse)
async def dashboard(request: Request):
    """
    Trading Dashboard sayfasƒ± - Jinja2 template ile render edilir
    """
    current_time = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")
    
    # Template'e g√∂nderebileceƒüin veriler (ileride geni≈ületebilirsin)
    context = {
        "request": request,               # Jinja2 i√ßin zorunlu
        "title": "ICT Smart Pro - Trading Dashboard",
        "now": current_time,
        "version": "5.0 Enterprise Edition",
        "status": "Aktif",
       
    }
    
    return templates.TemplateResponse(
        "dashboard.html",      # dosya adƒ± (templates/ klas√∂r√º i√ßinden)
        context
    )
    

# ========== STARTUP & SHUTDOWN HANDLERS ==========
startup_time = time.time()

@app.on_event("startup")
async def startup_event():
    logger.info("="*80)
    logger.info("üöÄ TRADING BOT BACKEND v5.0 STARTED")
    logger.info("="*80)
    logger.info(f"Environment: {Config.ENV}")
    logger.info(f"Debug Mode: {Config.DEBUG}")
    logger.info(f"Real Data Only Policy: {'ENFORCED' if not Config.ALLOW_SYNTHETIC_DATA else 'DISABLED'}")
    logger.info(f"Minimum Exchanges Required: {Config.REQUIRE_MIN_EXCHANGES}")
    logger.info(f"Minimum Candles for Analysis: {Config.MIN_CANDLES_FOR_ANALYSIS}")
    logger.info(f"ML Libraries Available: {ML_AVAILABLE}")
    logger.info(f"Cache TTL: {Config.CACHE_TTL_SECONDS} seconds")
    logger.info(f"Circuit Breaker Threshold: {Config.CIRCUIT_BREAKER_FAILURE_THRESHOLD} failures")
    logger.info("="*80)
    
    # Warm up exchange connections
    async with price_fetcher as fetcher:
        health = fetcher.get_exchange_health()
        healthy_count = health["healthy_exchanges"]
        total_count = health["total_exchanges"]
        logger.info(f"Exchange Health: {healthy_count}/{total_count} exchanges operational")
        
        if healthy_count < total_count * 0.7:
            logger.warning("‚ö†Ô∏è Less than 70% of exchanges are healthy - market data may be limited")
        else:
            logger.info("‚úÖ Exchange connectivity verified")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("="*80)
    logger.info("üõë TRADING BOT BACKEND SHUTTING DOWN")
    logger.info("="*80)
    logger.info(f"Uptime: {time.time() - startup_time:.0f} seconds")
    logger.info("Flushing logs and closing connections...")
    logger.info("Shutdown complete")
    logger.info("="*80)

# ========== MAIN ENTRY POINT ==========
if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")
    workers = int(os.getenv("WEB_CONCURRENCY", 4))
    
    logger.info(f"üì° Starting HTTP server on {host}:{port} with {workers} workers")
    logger.info(f"Environment: {Config.ENV} | Debug: {Config.DEBUG}")
    logger.info("Press Ctrl+C to stop")
    
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        workers=workers,
        log_level="info",
        access_log=False,
        reload=Config.DEBUG,
        timeout_keep_alive=30
    )
